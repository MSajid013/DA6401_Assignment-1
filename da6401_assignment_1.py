# -*- coding: utf-8 -*-
"""DA6401_Assignment-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhmBYPYbEqT_rTpvsVO9oeQsFoTV7sKG
"""

!pip install wandb

import wandb
wandb.login()

"""## Question-1"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow.keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split

# Initialize WandB project
wandb.init(project='DA6401_Assignment-1')

# Load Fashion-MNIST dataset (train and test sets)
(train_img, train_lbl), (test_img, test_lbl) = fashion_mnist.load_data()

# Define class names for labels
class_names = ['Top/Tshirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle-boot']

# Select one sample image for each class
samples = {}
for i in range(100):  # Loop through first 36 images
  lbl = train_lbl[i]  # Get label of current image
  if lbl not in samples:  # If label is not already added, add it
    samples[lbl] = train_img[i]
  if len(samples) == 10:  # Stop when all 10 classes have at least one image
    break

# Convert dictionary to lists for easy indexing
sample_lbl = list(samples.keys())
sample_img = list(samples.values())

# Function to plot 7 random sample images
def plot_sample(step):
  fig, axes = plt.subplots(1, 7, figsize=(14, 5))  # Create figure with 7 subplots
  chosen_indices = np.random.choice(len(sample_img), 7, replace=True)  # Randomly select 7 images

  for ax, idx in zip(axes.flatten(), chosen_indices):
    ax.imshow(sample_img[idx], cmap='gray')  # Display image
    ax.set_title(class_names[sample_lbl[idx]])  # Set title with class name
    ax.axis('off')  # Remove axis for cleaner view

  plt.tight_layout()
  plt.show()
  return fig

# Log images to WandB for visualization
wandb.log({"Examples": [wandb.Image(plot_sample(0), caption="Step 0"),
    wandb.Image(plot_sample(1), caption="Step 1"), wandb.Image(plot_sample(2), caption="Step 2")]})

# Finish WandB run
wandb.finish()

"""## Question-2 : Feedforward Neural Network

"""

# Function to apply the chosen activation function
def activation_function(x, func_type):
    """Return the output of the specified activation function."""
    if func_type == 'sigmoid':
        return 1 / (1+np.exp(-x))
    elif func_type == 'relu':
        return np.maximum(0, x)
    else:
        return np.tanh(x)

# Function to apply the corresponding activation derivative
def activation_derivative(x, func_type):
    """Return the derivative of the specified activation function."""
    if func_type == 'sigmoid':
        return (1/(1+np.exp(-x))) * (1-(1/(1+np.exp(-x))))
    elif func_type == 'relu':
        return np.where(x<=0, 0, 1)
    else:
        return 1 - (np.tanh(x))**2

# Softmax function for multi-class classification
def softmax(x):
    """Compute the softmax function to convert logits into probabilities."""
    x = x - np.max(x)  # Normalize to prevent numerical instability
    return np.exp(x) / np.sum(np.exp(x), axis=0)

# Initialize network weights and biases
def initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim):
    """ Initialize the network parameters (weights and biases) using the specified initialization method.
    Args:
        num_layers (int): Number of hidden layers.
        num_neurons (int): Number of neurons in each hidden layer.
        weight_init (str): Weight initialization method ('random' or 'xavier').
        input_dim (int): Number of input features.
        output_dim (int): Number of output classes.

    Returns:
        dict: Dictionary containing initialized weights and biases.
    """
    layer_sizes = [input_dim] + [num_neurons] * num_layers + [output_dim]  # Define layer sizes
    parameters = {}

    if weight_init == 'random':
        # Initialize weights and biases randomly
        for i in range(1, num_layers + 2):
            parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1])
            parameters[f'b{i}'] = np.random.randn(layer_sizes[i], 1)
    elif weight_init == 'xavier':
        # Initialize weights using Xavier initialization
        for i in range(1, num_layers + 2):
            parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2 / layer_sizes[i-1])
            parameters[f'b{i}'] = np.random.randn(layer_sizes[i], 1) * np.sqrt(2 / layer_sizes[i-1])

    return parameters

# Forward propagation
def forward_propagation(inputs, params, activation, num_layers, input_dim):
    """ Perform forward propagation through the neural network.
    Args:
        inputs (numpy array): Input data.
        params (dict): Dictionary containing weights and biases.
        activation (str): Activation function to use.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.

    Returns:
        tuple: Activations, layer outputs, and final predictions.
    """
    activations, layer_outputs = {}, {}
    activations['a0'] = np.zeros((input_dim, 1))  # Initialize first activation (not used)
    layer_outputs['h0'] = inputs  # Input layer

    # Forward propagate through hidden layers
    for i in range(1, num_layers + 1):
        activations[f'a{i}'] = np.dot(params[f'W{i}'], layer_outputs[f'h{i-1}']) + params[f'b{i}']
        layer_outputs[f'h{i}'] = activation_function(activations[f'a{i}'], activation)

    # Compute final layer output (softmax for multi-class classification)
    activations[f'a{num_layers+1}'] = np.dot(params[f'W{num_layers+1}'], layer_outputs[f'h{num_layers}']) + params[f'b{num_layers+1}']
    predictions = softmax(activations[f'a{num_layers+1}'])

    return activations, layer_outputs, predictions

# Load Fashion MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Preprocess data
x_train = x_train.reshape(x_train.shape[0], -1).T / 255.0  # Normalize and flatten
x_test = x_test.reshape(x_test.shape[0], -1).T / 255.0

# Convert labels to one-hot encoding
y_train_one_hot = np.eye(10)[y_train].T
y_test_one_hot = np.eye(10)[y_test].T

# Initialize neural network parameters
num_layers = 5
num_neurons = 128
weight_init = 'xavier'
input_dim = 784
output_dim = 10
activation = 'sigmoid'
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)

# Run forward propagation on a sample
sample_input = x_train[:, :1]  # Use a single sample
target_output = y_train_one_hot[:, :1]
acts, out, pred = forward_propagation(sample_input, params, activation, num_layers, input_dim)

# Print predicted probabilities
print("Predicted probabilities:", pred.T)

"""## Question-3 : Backpropagation with Optimisation Algorithms"""

# Compute cross-entropy loss
def cross_entropy_loss(y_true, y_pred, params, lambda_reg, num_layers):
    """
    Compute the cross-entropy loss for multi-class classification.

    Args:
        y_true (numpy array): One-hot encoded true labels.
        y_pred (numpy array): Predicted probability distribution.
        lambda_reg (float): Regularization parameter
        num_layers (int): Number of hidden layers.

    Returns:
        float: Cross-entropy loss.
    """
    epsilon = 1e-15  # Prevents log(0)
    y_pred = np.clip(y_pred, epsilon, 1-epsilon)  # Avoid numerical issues
    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=0))
    reg_term = (lambda_reg / 2) * sum(np.sum(params[f'W{i}']**2) for i in range(1, num_layers+2)) # L2 regularization (weight decay)
    return loss + reg_term

# Backward propagation
def backward_propagation(inputs, labels, activation, num_layers, params, input_dim,lambda_reg):
    """
    Perform backward propagation to compute gradients for weight and bias updates.
    Args:
        inputs (numpy array): Input data.
        labels (numpy array): True labels in one-hot encoding.
        activation (str): Activation function used in hidden layers.
        num_layers (int): Number of hidden layers.
        params (dict): Dictionary containing weights and biases.
        input_dim (int): Number of input features.
        lambda_reg (float): Regularization parameter.

    Returns:
        dict: Gradients of weights and biases.
    """
    activations, layer_outputs, predictions = forward_propagation(inputs, params, activation, num_layers, input_dim)
    gradients, backprops = {}, {}

    # Compute loss gradient w.r.t final layer activation
    backprops[f'a{num_layers+1}'] = predictions-labels

    # Backpropagate through layers
    for i in range(num_layers+1, 0, -1):
        gradients[f'W{i}'] = np.dot(backprops[f'a{i}'], layer_outputs[f'h{i-1}'].T) + lambda_reg *params[f'W{i}']
        gradients[f'b{i}'] = backprops[f'a{i}']
        backprops[f'h{i-1}'] = np.dot(params[f'W{i}'].T, backprops[f'a{i}'])
        backprops[f'a{i-1}'] = backprops[f'h{i-1}'] * activation_derivative(activations[f'a{i-1}'], activation)

    return gradients

# Compute accuracy on test data
def compute_accuracy(X_test, Y_test, trained_params, activation, num_layers,input_dim):
    """
    Compute classification accuracy on test data.

    Args:
        X_test (numpy array): Test data.
        Y_test (numpy array): True labels in one-hot encoding.
        trained_params (dict): Trained weights and biases.
        activation (str): Activation function used in hidden layers.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.

    Returns:
        float: Accuracy of the model on test data.
    """
    num_samples = X_test.shape[0]
    correct = 0

    # Iterate over all test samples
    for i in range(num_samples):
        input_sample = X_test[i, :].reshape(-1, 1)
        act, out, pred = forward_propagation(input_sample,trained_params, activation, num_layers, input_dim)

        # Check if predicted class matches true label
        if np.argmax(Y_test[i, :].reshape(-1, 1)) == np.argmax(pred):
            correct += 1

    return correct/num_samples  # Return accuracy as a fraction

"""Stochastic Gradient Descent"""

def stochastic_gd(learning_rate, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_layers, num_neurons, weight_init, batch_size, input_dim, output_dim, weight_decay):
    """
    Perform stochastic gradient descent (SGD) to train a neural network.
    Args:
        learning_rate (float): Learning rate for weight updates.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_valid (numpy array): Validation input data.
        Y_valid (numpy array): One-hot encoded validation labels.
        epochs (int): Number of training epochs.
        activation (str): Activation function used in hidden layers.
        num_layers (int): Number of hidden layers.
        num_neurons (int): Number of neurons per hidden layer.
        weight_init (str): Weight initialization method ('random' or 'xavier').
        batch_size (int): Number of samples per batch update.
        input_dim (int): Number of input features.
        output_dim (int): Number of output classes.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Trained network parameters (weights and biases).
    """
    # Initialize the neural network parameters (weights and biases)
    params = initialize_network(num_layers, num_neurons,weight_init, input_dim, output_dim)

    # List to store loss values over epochs for visualization
    losses = []

    # Get the number of training samples
    num_samples = X_train.shape[0]

    for epoch in range(epochs):  # Loop over the number of epochs
        indices = np.arange(num_samples)  # Create an array of sample indices
        np.random.shuffle(indices)  # shuffle the indices for stochastic updates

        # Dictionary to accumulate gradients for batch updates
        batch_updates = {key: np.zeros_like(value) for key, value in params.items()}

        for idx in range(num_samples):  # Loop over each training sample
            # Select one sample and reshape it into a column vector
            input_sample = X_train[indices[idx], :].reshape(-1, 1)
            label_sample = Y_train[indices[idx], :].reshape(-1, 1)

            # Perform backward propagation to compute gradients
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for batch update
            for k in range(1, num_layers + 2):
                batch_updates[f'W{k}'] += gradients[f'W{k}']
                batch_updates[f'b{k}'] += gradients[f'b{k}']

            # Perform parameter update after processing 'batch_size' samples
            if (idx + 1) % batch_size == 0:
                for i in range(1, num_layers + 2):
                    params[f'W{i}'] -= learning_rate * batch_updates[f'W{i}']
                    params[f'b{i}'] -= learning_rate * batch_updates[f'b{i}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot training loss over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Over Epochs')
    plt.show()

    return params  # Return the trained network parameters

"""Momentum Based Gradient Descent"""

def momentum_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, beta,weight_decay):
    """
    Implements the Momentum Optimizer for training a neural network.
        u_t = β * u_(t-1) + ∇w_t
        w_(t+1) = w_t - η * u_t

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_valid (numpy array): Validation input data.
        Y_valid (numpy array): One-hot encoded validation labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        beta (float): Momentum coefficient.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    losses = []  # List to store the training loss over epochs
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize velocity for momentum updates (same shape as parameters)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate over epochs
        num_processed = 0  # Counter for processed samples
        epoch_loss = 0  # Initialize loss for the current epoch
        num_batches = 0  # Count number of batches processed
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}  # Accumulate gradients

        for sample_idx in range(num_samples):  # Loop through all training samples
            num_processed += 1  # Increment processed sample counter

            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            # Update weights and biases once the batch is complete
            if num_processed % batch_size == 0:
                num_batches += 1  # Increment batch count

                for layer in range(1, num_layers + 2):
                    # Compute velocity update using momentum
                    velocity[f'W{layer}'] = beta * prev_v[f'W{layer}'] + grad_accu[f'W{layer}']
                    velocity[f'b{layer}'] = beta * prev_v[f'b{layer}'] + grad_accu[f'b{layer}']


                    # Apply momentum-based update to parameters
                    params[f'W{layer}'] -= learning_rate * velocity[f'W{layer}']
                    params[f'b{layer}'] -= learning_rate * velocity[f'b{layer}']

                # Store previous velocity values for the next batch update
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

        # Compute training accuracy at the end of the epoch
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_predictions = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_predictions, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot the loss function over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss over Epochs')
    plt.show()

    return params  # Return the trained parameters

"""Nesterov Accelerated Gradient Descent"""

def nag_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, beta,weight_decay):
    """
    Implements Nesterov Accelerated Gradient (NAG) optimizer for training a neural network.
        u_t = β * u_(t-1) + ∇(w_t - η*β * u_(t-1))
        w_(t+1) = w_t - η*u_t

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_valid (numpy array): Validation input data.
        Y_valid (numpy array): One-hot encoded validation labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        beta (float): Momentum coefficient.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    losses = []  # List to store training loss for each epoch
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize velocity for momentum updates (same shape as parameters)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate over epochs
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}  # Accumulate gradients
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize loss for current epoch

        for sample_idx in range(num_samples):  # Iterate over all training samples
            num_processed += 1  # Increment processed sample counter

            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Look ahead by applying momentum before computing gradients
            for layer in range(1, num_layers + 2):
                params[f'W{layer}'] -= learning_rate * beta * prev_v[f'W{layer}']
                params[f'b{layer}'] -= learning_rate * beta * prev_v[f'b{layer}']

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                num_batches += 1  # Increment batch count

                for layer in range(1, num_layers + 2):
                    # Compute velocity update using Nesterov's momentum
                    velocity[f'W{layer}'] = beta * prev_v[f'W{layer}'] + grad_accu[f'W{layer}']
                    velocity[f'b{layer}'] = beta * prev_v[f'b{layer}'] + grad_accu[f'b{layer}']


                    # Apply velocity-based update to parameters
                    params[f'W{layer}'] -= learning_rate * velocity[f'W{layer}']
                    params[f'b{layer}'] -= learning_rate * velocity[f'b{layer}']

                # Store previous velocity values for the next batch update
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot the loss function over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss over Epochs')
    plt.show()

    return params  # Return the trained parameters

"""RMSPROP"""

def rmsprop_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay):
    """
    Implements the RMSprop (Root Mean Square Propagation) optimizer for training a neural network.
        v_t = β * v_(t-1) + (1-β) * (∇w_t)^2
        w_(t+1) = w_t - η*(∇w_t / (√v_t + ε))

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_valid (numpy array): Validation input data.
        Y_valid (numpy array): One-hot encoded validation labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        epsilon (float): Small constant to avoid division by zero.
        beta (float, optional): Decay rate for the moving average of squared gradients.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    losses = []  # List to store training loss for each epoch
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize velocity (moving average of squared gradients) for each parameter
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate through epochs
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize loss for the current epoch

        # Initialize gradient accumulator for batch updates
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):  # Iterate through all training samples
            num_processed += 1  # Increment processed sample counter

            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                num_batches += 1  # Increment batch count

                for layer in range(1, num_layers + 2):
                    # Compute moving average of squared gradients (RMSprop update)
                    velocity[f'W{layer}'] = beta*prev_v[f'W{layer}'] + (1-beta)*(grad_accu[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta*prev_v[f'b{layer}'] + (1-beta)*(grad_accu[f'b{layer}']**2)


                    # Apply RMSprop update rule to parameters
                    params[f'W{layer}'] -= (learning_rate/(np.sqrt(velocity[f'W{layer}']+epsilon))) * grad_accu[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate/(np.sqrt(velocity[f'b{layer}']+epsilon))) * grad_accu[f'b{layer}']

                # Store previous velocity values for the next batch update
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot the loss function over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss over Epochs')
    plt.show()

    return params  # Return the trained parameters

"""ADAM"""

def adam_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Adam (Adaptive Moment Estimation) optimizer for training a neural network.
        m_t = β1*m_(t-1) + (1-β1)*∇w_t
        hat{m_t} = m_t / (1-β1^t)
        v_t = β2*v_(t-1) + (1-β2)*(∇w_t)^2
        hat{v_t} = v_t / (1-β2^t)
        w_(t+1) = w_t - η*(hat{m_t} / (√(hat{v_t}) +ε) )

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_valid (numpy array): validation input data.
        Y_valid (numpy array): One-hot encoded validation labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        epsilon (float): Small constant to avoid division by zero.
        beta1 (float): exponential decay rate for first moment estimates.
        beta2 (float): exponential decay rate for second moment estimates.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    losses = []  # List to store loss values for each epoch
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize first moment vector (momentum)
    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    # Initialize second moment vector (velocity - moving average of squared gradients)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate over epochs
        time_step = 0  # Time step counter for bias correction
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize epoch loss

        # Initialize gradient accumulator for batch updates
        gradient_accumulator = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):  # Iterate through all training samples
            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                gradient_accumulator[f'W{layer}'] += gradients[f'W{layer}']
                gradient_accumulator[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1  # Increment processed sample counter

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                time_step += 1  # Increment time step for bias correction
                num_batches += 1  # Increment batch count

                # Compute biased first moment estimate (momentum)
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1*prev_m[f'W{layer}'] + (1-beta1)*gradient_accumulator[f'W{layer}']
                    momentum[f'b{layer}'] = beta1*prev_m[f'b{layer}'] + (1-beta1)*gradient_accumulator[f'b{layer}']

                # Compute bias-corrected first moment estimate
                    m_hat[f'W{layer}'] = momentum[f'W{layer}'] / (1 - np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}'] / (1 - np.power(beta1, time_step))

                    # Update previous momentum values
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute biased second moment estimate (velocity)
                    velocity[f'W{layer}'] = beta2*prev_v[f'W{layer}'] + (1-beta2)*(gradient_accumulator[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2*prev_v[f'b{layer}'] + (1-beta2)*(gradient_accumulator[f'b{layer}']**2)

                # Compute bias-corrected second moment estimate
                    v_hat[f'W{layer}'] = velocity[f'W{layer}']/(1 - np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}']/(1 - np.power(beta2, time_step))

                    # Update previous velocity values
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Apply Adam update rule to parameters
                    params[f'W{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'W{layer}']) + epsilon)) *m_hat[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'b{layer}']) + epsilon)) *m_hat[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot the loss function over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss over Epochs')
    plt.show()

    return params  # Return the trained parameters

"""NADAM"""

def nadam_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Nadam (Nesterov-accelerated Adaptive Moment Estimation) optimization algorithm.

    Args:
        learning_rate: The step size for parameter updates.
        params: Dictionary containing the neural network parameters.
        X_train, Y_train: Training data and corresponding labels.
        X_valid, Y_valid: Validation data and corresponding labels.
        activation: Activation function used in the network.
        epochs: Number of training iterations.
        num_layers: Number of hidden layers.
        input_dim: Input dimension of the data.
        batch_size: Number of samples per batch update.
        epsilon: Small value to avoid division by zero.
        beta1: Decay rate for the first moment estimate (momentum term).
        beta2: Decay rate for the second moment estimate (velocity term).
        weight_decay (float): L2 regularization strength.

    Returns:
        Updated parameters after training.
    """
    losses = []
    num_samples = X_train.shape[0]

    # Initialize velocity and momentum terms for all parameters
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):
        time_step = 0
        num_processed = 0
        num_batches = 0
        epoch_loss = 0
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):
            # Forward pass for a single training sample
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients over the batch
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1
            if num_processed % batch_size == 0:
                time_step += 1
                num_batches += 1

                # Compute momentum estimates
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1 *prev_m[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']
                    momentum[f'b{layer}'] = beta1 *prev_m[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']

                # Bias-corrected momentum
                    m_hat[f'W{layer}'] = momentum[f'W{layer}']/(1-np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}']/(1-np.power(beta1, time_step))
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute velocity estimates
                    velocity[f'W{layer}'] = beta2 * prev_v[f'W{layer}'] + (1-beta2) * (grad_accu[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2 * prev_v[f'b{layer}'] + (1-beta2) * (grad_accu[f'b{layer}']**2)

                # Bias-corrected velocity
                    v_hat[f'W{layer}'] = velocity[f'W{layer}'] /(1-np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}'] /(1-np.power(beta2, time_step))
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Update parameters using Nadam update rule
                    params[f'W{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'W{layer}'])+epsilon)) * (beta1*m_hat[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']/(1-np.power(beta1, time_step)))
                    params[f'b{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'b{layer}'])+epsilon)) * (beta1*m_hat[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']/(1-np.power(beta1, time_step)))

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        losses.append(train_loss)  # Store loss for visualization

        # Compute validation accuracy
        val_acc = compute_accuracy(X_valid, Y_valid, params, activation, num_layers, input_dim)

        # Compute validation loss using cross-entropy
        act, out, val_preds = forward_propagation(X_valid.T, params, activation, num_layers, input_dim)
        val_loss = cross_entropy_loss(Y_valid.T, val_preds, params, weight_decay, num_layers)


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_Loss: {train_loss:.4f}, Train_Accuracy: {train_acc * 100:.2f}%, Val_Loss: {val_loss:.4f}, Val_Accuracy: {val_acc * 100:.2f}%")

        # Log results using Weights & Biases
        wandb.log({'Train_loss': train_loss,
                   'Train_accuracy': train_acc * 100,
                   'Val_loss': val_loss,
                   'Val_accuracy': val_acc * 100,
                   'Epoch': epoch})

    # Plot the loss function over epochs
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss over Epochs')
    plt.show()

    return params

"""## Question-4"""

# Load the Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Split the training data into training and validation sets (90% train, 10% validation)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

# Flatten the images (convert 28x28 matrices into 1D arrays of size 784)
# Normalize pixel values to the range [0,1] by dividing by 255
X_train = X_train.reshape(X_train.shape[0], -1) / 255
X_valid = X_valid.reshape(X_valid.shape[0], -1) / 255
X_test = X_test.reshape(X_test.shape[0], -1) / 255

def one_hot_encod(arr):
    """ Convert class labels to one-hot encoded vectors.
    Args: arr (numpy array): Array of class labels (e.g, [0, 1, 2, ..., 9])

    Returns: numpy array: One-hot encoded matrix of shape (len(arr), 10)
    """
    mat = np.zeros((len(arr), 10))  # Create a matrix of zeros with shape (num_samples, num_classes)
    for i in range(len(arr)):
        mat[i, arr[i]] = 1  # Set the corresponding class index to 1
    return mat

# Convert labels to one-hot encoding for training, validation, and test sets
Y_train = one_hot_encod(y_train)
Y_valid = one_hot_encod(y_valid)
Y_test = one_hot_encod(y_test)

def train_model(X_train,Y_train, X_valid, Y_valid, input_dim,output_dim, learning_rate=0.1, optimizer='sgd', regularization=0.0, epochs=1, activation='sigmoid',
                num_layers=1, num_neurons=4, weight_init='random', batch_size=4, epsilon=0.000001, beta=0.5, beta1=0.5, beta2=0.5):
    """
    Train a neural network model using the specified optimizer.

    Parameters:
    - X_train, Y_train: Training data and labels.
    - X_valid, Y_valid: Validation data and labels.
    - input_dim: Input feature dimension.
    - output_dim: Output dimension (number of classes for classification).
    - learning_rate: The learning rate for optimization.
    - optimizer: The optimization algorithm to use ('sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam').
    - regularization (weight_decay): L2 Regularization term.
    - epochs: Number of training epochs.
    - activation: Activation function to use.
    - num_layers: Number of hidden layers in the network.
    - num_neurons: Number of neurons in each hidden layer.
    - weight_init: Weight initialization method.
    - batch_size: Number of samples per training batch.
    - epsilon: Small constant to prevent division by zero.
    - beta, beta1, beta2: Hyperparameters for momentum-based optimizers.

    Returns:
    - params: The trained model parameters.
    """

    # Initialize the network with random weights based on the chosen weight initialization method
    params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)

    # Select the appropriate optimizer and train the model
    if optimizer == 'sgd':
        # Stochastic Gradient Descent
        params = stochastic_gd(learning_rate, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_layers, num_neurons, weight_init, batch_size, input_dim, output_dim,regularization)
    elif optimizer == 'momentum':
        # Momentum based Gradient Descent
        params = momentum_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, beta,regularization)
    elif optimizer == 'nesterov':
        # Nesterov Accelerated Gradient Descent (NAG)
        params = nag_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, beta,regularization)
    elif optimizer == 'rmsprop':
        # RMSprop optimizer
        params = rmsprop_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,regularization)
    elif optimizer == 'adam':
        # Adam optimizer (Adaptive Moment Estimation)
        params = adam_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,regularization)
    elif optimizer == 'nadam':
        # Nadam optimizer (Adam with Nesterov momentum)
        params = nadam_optimizer(learning_rate, params, X_train, Y_train, X_valid, Y_valid, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,regularization)

    return params

# Define the hyperparameter sweep configuration
sweep_config = {
    'method': 'bayes',  # Use Bayesian optimization for efficient hyperparameter tuning
    'name': 'sweep cross entropy now',  # Name of the sweep for tracking
    'metric': {'name': 'Val_accuracy',  # The metric to optimize
               'goal': 'maximize'},  # The objective is to maximize Training accuracy
    'parameters': {
        'epochs': {'values': [5, 10]},  # Number of training epochs
        'num_layers': {'values': [3, 4, 5]},  # Number of hidden layers
        'num_neurons': {'values': [32, 64, 128]},  # Neurons per layer
        'regularization': {'values': [0, 0.0005, 0.5]},  # L2 regularization strength
        'learning_rate': {'values': [1e-3, 1e-4]},  # Learning rate values
        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},  # Optimizers to test
        'batch_size': {'values': [16, 32, 64]},  # Batch sizes for training
        'weight_init': {'values': ['random', 'xavier']},  # Weight initialization methods
        'activation': {'values': ['sigmoid', 'relu', 'tanh']}  # Activation functions to test
    }
}

# Initialize the sweep and get the sweep ID
sweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_Assignment-1')
# Creates a new sweep in the project 'DA6401_Assignment-1' and returns a sweep ID

def main():
    '''
    WandB calls the main function each time with a different combination of hyperparameters.

    We retrieve these values using `wandb.config` and use them to train the model.
    '''

    with wandb.init() as run:
        # Generate a unique run name based on the hyperparameter values
        run_name = "ac-" + (wandb.config.activation) + \
                   "_nn-" + str(wandb.config.num_neurons) + \
                   "_epc-" + str(wandb.config.epochs) + \
                   "_nl-" + str(wandb.config.num_layers) + \
                   "_wd-" + str(wandb.config.regularization) + \
                   "_lr-" + str(wandb.config.learning_rate) + \
                   "_opt-" + str(wandb.config.optimizer) + \
                   "_bs-" + str(wandb.config.batch_size) + \
                   "_wi-" + str(wandb.config.weight_init)

        # Assign the generated name to the WandB run
        wandb.run.name = run_name

        # Train the model with the current hyperparameter configuration
        train_model(
            X_train, Y_train,            # Training data
            X_valid, Y_valid,            # Validation data
            784, 10,                     # Input and output dimensions (Fashion MNIST: 784 input features, 10 classes)
            wandb.config.learning_rate,  # Learning rate
            wandb.config.optimizer,      # Optimizer
            wandb.config.regularization, # Regularization term
            wandb.config.epochs,         # Number of epochs
            wandb.config.activation,     # Activation function
            wandb.config.num_layers,     # Number of hidden layers
            wandb.config.num_neurons,    # Neurons per layer
            wandb.config.weight_init,    # Weight initialization method
            wandb.config.batch_size,     # Batch size
            epsilon=0.0001,                # Small constant for numerical stability
            beta=0.9, beta1=0.9, beta2=0.99  # Hyperparameters for momentum-based optimizers
        )

# Run the sweep agent to execute `main` function multiple times
wandb.agent(sweep_id, function=main, count=100)  # Calls `main` function 100 times with different hyperparameter sets

# Mark the end of the WandB run
wandb.finish()

# Define the hyperparameter sweep configuration
sweep_config = {
    'method': 'bayes',  # Use Bayesian optimization for efficient hyperparameter tuning
    'name': 'sweep cross entropy later',  # Name of the sweep for tracking
    'metric': {'name': 'Val_accuracy',  # The metric to optimize
               'goal': 'maximize'},  # The objective is to maximize validation accuracy
    'parameters': {
        'epochs': {'values': [10]},  # Number of training epochs
        'num_layers': {'values': [4,5]},  # Number of hidden layers
        'num_neurons': {'values': [64,128]},  # Neurons per layer
        'regularization': {'values': [0,0.0005]},  # L2 regularization strength
        'learning_rate': {'values': [1e-3,1e-4]},  # Learning rate values
        'optimizer': {'values': ['rmsprop', 'adam', 'nadam']},  # Optimizers to test
        'batch_size': {'values': [32,64]},  # Batch sizes for training
        'weight_init': {'values': ['xavier']},  # Weight initialization methods
        'activation': {'values': ['sigmoid', 'relu']}  # Activation functions to test
    }
}

# Initialize the sweep and get the sweep ID
sweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_Assignment-1')
# Creates a new sweep in the project 'DA6401_Assignment-1' and returns a sweep ID

def main():
    '''
    WandB calls the main function each time with a different combination of hyperparameters.

    We retrieve these values using `wandb.config` and use them to train the model.
    '''

    with wandb.init() as run:
        # Generate a unique run name based on the hyperparameter values
        run_name = "ac-" + (wandb.config.activation) + \
                   "_nn-" + str(wandb.config.num_neurons) + \
                   "_epc-" + str(wandb.config.epochs) + \
                   "_nl-" + str(wandb.config.num_layers) + \
                   "_wd-" + str(wandb.config.regularization) + \
                   "_lr-" + str(wandb.config.learning_rate) + \
                   "_opt-" + str(wandb.config.optimizer) + \
                   "_bs-" + str(wandb.config.batch_size) + \
                   "_wi-" + str(wandb.config.weight_init)

        # Assign the generated name to the WandB run
        wandb.run.name = run_name

        # Train the model with the current hyperparameter configuration
        train_model(
            X_train, Y_train,            # Training data
            X_valid, Y_valid,            # Validation data
            784, 10,                     # Input and output dimensions (Fashion MNIST: 784 input features, 10 classes)
            wandb.config.learning_rate,  # Learning rate
            wandb.config.optimizer,      # Optimizer
            wandb.config.regularization,   # Regularization term
            wandb.config.epochs,         # Number of epochs
            wandb.config.activation,     # Activation function
            wandb.config.num_layers,     # Number of hidden layers
            wandb.config.num_neurons,    # Neurons per layer
            wandb.config.weight_init,    # Weight initialization method
            wandb.config.batch_size,     # Batch size
            epsilon=0.0001,                # Small constant for numerical stability
            beta=0.9, beta1=0.9, beta2=0.99  # Hyperparameters for momentum-based optimizers
        )

# Run the sweep agent to execute `main` function multiple times
wandb.agent(sweep_id, function=main, count=50)  # Calls `main` function 50 times with different hyperparameter sets

# Mark the end of the WandB run
wandb.finish()

"""## Question-7 : Test Accuracy and Confusion Matrix

I am getting higher accuracy with RMSPROP, ADAM and NADAM. So I am using these optimizers to find test accuracy.
"""

def nadam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Nadam (Nesterov-accelerated Adaptive Moment Estimation) optimization algorithm.

    Args:
        learning_rate: The step size for parameter updates.
        params: Dictionary containing the neural network parameters.
        X_train, Y_train: Training data and corresponding labels.
        X_test, Y_test: Testing data and corresponding labels.
        activation: Activation function used in the network.
        epochs: Number of training iterations.
        num_layers: Number of hidden layers.
        input_dim: Input dimension of the data.
        batch_size: Number of samples per batch update.
        epsilon: Small value to avoid division by zero.
        beta1: Decay rate for the first moment estimate (momentum term).
        beta2: Decay rate for the second moment estimate (velocity term).
        weight_decay (float): L2 regularization strength.

    Returns:
        Updated parameters after training.
    """
    train_losses = []
    test_losses = []
    num_samples = X_train.shape[0]

    # Initialize velocity and momentum terms for all parameters
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):
        time_step = 0
        num_processed = 0
        num_batches = 0
        epoch_loss = 0
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):
            # Forward pass for a single training sample
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients over the batch
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1
            if num_processed % batch_size == 0:
                time_step += 1
                num_batches += 1

                # Compute momentum estimates
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1 *prev_m[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']
                    momentum[f'b{layer}'] = beta1 *prev_m[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']

                # Bias-corrected momentum
                    m_hat[f'W{layer}'] = momentum[f'W{layer}'] / (1-np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}'] / (1-np.power(beta1, time_step))
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute velocity estimates
                    velocity[f'W{layer}'] = beta2 * prev_v[f'W{layer}'] + (1-beta2) * (grad_accu[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2 * prev_v[f'b{layer}'] + (1-beta2) * (grad_accu[f'b{layer}']**2)

                # Bias-corrected velocity
                    v_hat[f'W{layer}'] = velocity[f'W{layer}'] / (1-np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}'] / (1-np.power(beta2, time_step))
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Update parameters using Nadam update rule
                    params[f'W{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'W{layer}'])+epsilon)) * (beta1*m_hat[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']/(1-np.power(beta1, time_step)))
                    params[f'b{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'b{layer}'])+epsilon)) * (beta1*m_hat[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']/(1-np.power(beta1, time_step)))

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)  # Store loss for visualization

        # Compute test accuracy
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute test loss using cross-entropy
        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = cross_entropy_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)  # Store loss for visualization


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")


    # Plot the loss function over epochs
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.show()

    return params

def adam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Adam (Adaptive Moment Estimation) optimizer for training a neural network.
        m_t = β1*m_(t-1) + (1-β1)*∇w_t
        hat{m_t} = m_t / (1-β1^t)
        v_t = β2*v_(t-1) + (1-β2)*(∇w_t)^2
        hat{v_t} = v_t / (1-β2^t)
        w_(t+1) = w_t - η*(hat{m_t} / (√(hat{v_t}) +ε) )

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_test (numpy array): Test input data.
        Y_test (numpy array): One-hot encoded test labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        epsilon (float): Small constant to avoid division by zero.
        beta1 (float): Exponential decay rate for first moment estimates.
        beta2 (float): Exponential decay rate for second moment estimates.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    train_losses = []  # List to store loss values for each epoch
    test_losses = []
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize first moment vector (momentum)
    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    # Initialize second moment vector (velocity - moving average of squared gradients)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate over epochs
        time_step = 0  # Time step counter for bias correction
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize epoch loss

        # Initialize gradient accumulator for batch updates
        gradient_accumulator = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):  # Iterate through all training samples
            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                gradient_accumulator[f'W{layer}'] += gradients[f'W{layer}']
                gradient_accumulator[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1  # Increment processed sample counter

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                time_step += 1  # Increment time step for bias correction
                num_batches += 1  # Increment batch count

                # Compute biased first moment estimate (momentum)
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1*prev_m[f'W{layer}'] + (1-beta1)*gradient_accumulator[f'W{layer}']
                    momentum[f'b{layer}'] = beta1*prev_m[f'b{layer}'] + (1-beta1)*gradient_accumulator[f'b{layer}']

                # Compute bias-corrected first moment estimate
                    m_hat[f'W{layer}'] = momentum[f'W{layer}'] / (1 - np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}'] / (1 - np.power(beta1, time_step))

                    # Update previous momentum values
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute biased second moment estimate (velocity)
                    velocity[f'W{layer}'] = beta2*prev_v[f'W{layer}'] + (1-beta2)*(gradient_accumulator[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2*prev_v[f'b{layer}'] + (1-beta2)*(gradient_accumulator[f'b{layer}']**2)

                # Compute bias-corrected second moment estimate
                    v_hat[f'W{layer}'] = velocity[f'W{layer}'] / (1 - np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}'] / (1 - np.power(beta2, time_step))

                    # Update previous velocity values
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Apply Adam update rule to parameters
                    params[f'W{layer}'] -= (learning_rate / (np.sqrt(v_hat[f'W{layer}']) + epsilon)) *m_hat[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate / (np.sqrt(v_hat[f'b{layer}']) + epsilon)) *m_hat[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)  # Store loss for visualization

        # Compute test accuracy
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute test loss using cross-entropy
        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = cross_entropy_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)  # Store loss for visualization


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")

    # Plot the loss function over epochs
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.show()

    return params

def rmsprop_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay):
    """
    Implements the RMSprop (Root Mean Square Propagation) optimizer for training a neural network.
        v_t = β * v_(t-1) + (1-β) * (∇w_t)^2
        w_(t+1) = w_t - η*(∇w_t / (√v_t + ε))

    Args:
        learning_rate (float): Learning rate for parameter updates.
        params (dict): Dictionary containing neural network weights and biases.
        X_train (numpy array): Training input data.
        Y_train (numpy array): One-hot encoded training labels.
        X_test (numpy array): Test input data.
        Y_test (numpy array): One-hot encoded test labels.
        activation (str): Activation function used in hidden layers.
        epochs (int): Number of training epochs.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.
        batch_size (int): Number of samples per batch update.
        epsilon (float): Small constant to avoid division by zero.
        beta (float): Decay rate for the moving average of squared gradients.
        weight_decay (float): L2 regularization strength.

    Returns:
        dict: Updated network parameters after training.
    """

    train_losses = []  # List to store training loss for each epoch
    test_losses = []
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize velocity (moving average of squared gradients) for each parameter
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate through epochs
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize loss for the current epoch

        # Initialize gradient accumulator for batch updates
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):  # Iterate through all training samples
            num_processed += 1  # Increment processed sample counter

            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                num_batches += 1  # Increment batch count

                for layer in range(1, num_layers + 2):
                    # Compute moving average of squared gradients (RMSprop update)
                    velocity[f'W{layer}'] = beta*prev_v[f'W{layer}'] + (1-beta)*(grad_accu[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta*prev_v[f'b{layer}'] + (1-beta)*(grad_accu[f'b{layer}']**2)


                    # Apply RMSprop update rule to parameters
                    params[f'W{layer}'] -= (learning_rate / (np.sqrt(velocity[f'W{layer}'] + epsilon))) * grad_accu[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate / (np.sqrt(velocity[f'b{layer}'] + epsilon))) * grad_accu[f'b{layer}']

                # Store previous velocity values for the next batch update
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = cross_entropy_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)  # Store loss for visualization

        # Compute test accuracy
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute test loss using cross-entropy
        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = cross_entropy_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)  # Store loss for visualization


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")


    # Plot the loss function over epochs
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.show()

    return params

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=64
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=5
num_neurons=128
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)

# Nadam optimizer (Adam with Nesterov momentum)
parameters = nadam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

parameters = adam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

parameters = rmsprop_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay)

"""I found highest test accuracy with RMSprop, So I am using RMSprop optimizer for creating confusion matrix."""

# Load the Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Split the training data into training and validation sets (90% train, 10% validation)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

# Flatten the images (convert 28x28 matrices into 1D arrays of size 784)
# Normalize pixel values to the range [0,1] by dividing by 255
X_train = X_train.reshape(X_train.shape[0], -1) / 255
X_valid = X_valid.reshape(X_valid.shape[0], -1) / 255
X_test = X_test.reshape(X_test.shape[0], -1) / 255

def one_hot_encod(arr):
    """ Convert class labels to one-hot encoded vectors.
    Args: arr (numpy array): Array of class labels (e.g., [0, 1, 2, ..., 9])

    Returns: numpy array: One-hot encoded matrix of shape (len(arr), 10)
    """
    mat = np.zeros((len(arr), 10))  # Create a matrix of zeros with shape (num_samples, num_classes)
    for i in range(len(arr)):
        mat[i, arr[i]] = 1  # Set the corresponding class index to 1
    return mat

# Convert labels to one-hot encoding for training, validation, and test sets
Y_train = one_hot_encod(y_train)
Y_valid = one_hot_encod(y_valid)
Y_test = one_hot_encod(y_test)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import wandb
from sklearn.metrics import confusion_matrix

# Initialize wandb
wandb.init(project="DA6401_A-1_Confusion_matrix", name="confusion_matrix")

# Function to compute predictions for test set
def get_predictions(X_test, trained_params, activation, num_layers, input_dim):
    """
    Compute class predictions for the test set.

    Args:
        X_test (numpy array): Test data.
        trained_params (dict): Trained weights and biases.
        activation (str): Activation function.
        num_layers (int): Number of hidden layers.
        input_dim (int): Number of input features.

    Returns:
        numpy array: Predicted class labels.
    """
    predictions = []
    for i in range(X_test.shape[0]):
        input_sample = X_test[i, :].reshape(-1, 1)
        _, _, pred = forward_propagation(input_sample, trained_params, activation, num_layers, input_dim)
        predictions.append(np.argmax(pred))  # Convert probabilities to class labels
    return np.array(predictions)

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=64
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=5
num_neurons=128
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)
parameters = rmsprop_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay)

# Get true labels and predicted labels
y_true = np.argmax(Y_test, axis=1)  # Convert one-hot to class labels
y_pred = get_predictions(X_test, parameters, activation, num_layers, input_dim)

# Compute confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Define class names
classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")

# Save and log to wandb
plt.savefig("confusion_matrix.png")
wandb.log({"Confusion Matrix": wandb.Image("confusion_matrix.png")})

# Close wandb run
wandb.finish()

"""### Question-8 : Best Optimizers with Mean Squared error"""

def mse_loss(y_true, y_pred, params, weight_decay, num_layers):
    """
    Compute Mean Squared Error (MSE) loss with L2 regularization.

    Args:
        y_true (numpy array): One-hot encoded true labels.
        y_pred (numpy array): Predicted probability distribution.
        params (dict): Dictionary containing network weights.
        weight_decay (float): L2 regularization strength.
        num_layers (int): Number of layers.

    Returns:
        float: MSE loss with L2 regularization.
    """
    mse = np.mean(np.sum((y_true - y_pred) ** 2, axis=0))  # Compute standard MSE loss

    # Compute L2 regularization term
    l2_reg = (weight_decay / 2) * sum(np.sum(params[f'W{layer}'] ** 2) for layer in range(1, num_layers + 2))

    return mse + l2_reg


def rmsprop_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta, weight_decay):
    """
    Implements the RMSprop (Root Mean Square Propagation) optimizer for training a neural network using MSE loss.
    """
    train_losses = []
    test_losses = []
    num_samples = X_train.shape[0]

    # Initialize velocity (moving average of squared gradients)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):
        num_processed = 0
        num_batches = 0
        epoch_loss = 0

        # Initialize gradient accumulator
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):
            num_processed += 1

            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim, weight_decay)

            # Accumulate gradients
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            # Update weights and biases after a full batch
            if num_processed % batch_size == 0:
                num_batches += 1

                for layer in range(1, num_layers + 2):
                    velocity[f'W{layer}'] = beta * prev_v[f'W{layer}'] + (1 - beta) * (grad_accu[f'W{layer}'] ** 2)
                    velocity[f'b{layer}'] = beta * prev_v[f'b{layer}'] + (1 - beta) * (grad_accu[f'b{layer}'] ** 2)

                    params[f'W{layer}'] -= (learning_rate / (np.sqrt(velocity[f'W{layer}'] + epsilon))) * grad_accu[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate / (np.sqrt(velocity[f'b{layer}'] + epsilon))) * grad_accu[f'b{layer}']

                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

        # Compute accuracy
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute loss using MSE
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = mse_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)

        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = mse_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)

        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")

    # Plot loss curves
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs (MSE)')
    plt.legend()
    plt.show()

    return params

def adam_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Adam (Adaptive Moment Estimation) optimizer for training a neural network using mse.
    """

    train_losses = []  # List to store loss values for each epoch
    test_losses = []
    num_samples = X_train.shape[0]  # Number of training samples

    # Initialize first moment vector (momentum)
    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    # Initialize second moment vector (velocity - moving average of squared gradients)
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):  # Iterate over epochs
        time_step = 0  # Time step counter for bias correction
        num_processed = 0  # Counter for processed samples
        num_batches = 0  # Counter for number of batches
        epoch_loss = 0  # Initialize epoch loss

        # Initialize gradient accumulator for batch updates
        gradient_accumulator = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):  # Iterate through all training samples
            # Select a single training sample and reshape it into a column vector
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)

            # Compute gradients using backward propagation
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients for weight and bias updates
            for layer in range(num_layers + 1, 0, -1):
                gradient_accumulator[f'W{layer}'] += gradients[f'W{layer}']
                gradient_accumulator[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1  # Increment processed sample counter

            # Update weights and biases after processing a full batch
            if num_processed % batch_size == 0:
                time_step += 1  # Increment time step for bias correction
                num_batches += 1  # Increment batch count

                # Compute biased first moment estimate (momentum)
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1*prev_m[f'W{layer}'] + (1-beta1)*gradient_accumulator[f'W{layer}']
                    momentum[f'b{layer}'] = beta1*prev_m[f'b{layer}'] + (1-beta1)*gradient_accumulator[f'b{layer}']

                # Compute bias-corrected first moment estimate
                    m_hat[f'W{layer}'] = momentum[f'W{layer}'] / (1 - np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}'] / (1 - np.power(beta1, time_step))

                    # Update previous momentum values
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute biased second moment estimate (velocity)
                    velocity[f'W{layer}'] = beta2*prev_v[f'W{layer}'] + (1-beta2)*(gradient_accumulator[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2*prev_v[f'b{layer}'] + (1-beta2)*(gradient_accumulator[f'b{layer}']**2)

                # Compute bias-corrected second moment estimate
                    v_hat[f'W{layer}'] = velocity[f'W{layer}'] / (1 - np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}'] / (1 - np.power(beta2, time_step))

                    # Update previous velocity values
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Apply Adam update rule to parameters
                    params[f'W{layer}'] -= (learning_rate / (np.sqrt(v_hat[f'W{layer}']) + epsilon)) *m_hat[f'W{layer}']
                    params[f'b{layer}'] -= (learning_rate / (np.sqrt(v_hat[f'b{layer}']) + epsilon)) *m_hat[f'b{layer}']

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = mse_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)  # Store loss for visualization

        # Compute test accuracy
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute test loss using cross-entropy
        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = mse_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)  # Store loss for visualization


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")

    # Plot the loss function over epochs
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.show()

    return params

def nadam_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay):
    """
    Implements the Nadam (Nesterov-accelerated Adaptive Moment Estimation) optimization algorithm using mse.
    """
    train_losses = []
    test_losses = []
    num_samples = X_train.shape[0]

    # Initialize velocity and momentum terms for all parameters
    velocity = {key: np.zeros_like(value) for key, value in params.items()}
    prev_v = {key: np.zeros_like(value) for key, value in params.items()}
    v_hat = {key: np.zeros_like(value) for key, value in params.items()}

    momentum = {key: np.zeros_like(value) for key, value in params.items()}
    prev_m = {key: np.zeros_like(value) for key, value in params.items()}
    m_hat = {key: np.zeros_like(value) for key, value in params.items()}

    for epoch in range(epochs):
        time_step = 0
        num_processed = 0
        num_batches = 0
        epoch_loss = 0
        grad_accu = {key: np.zeros_like(value) for key, value in params.items()}

        for sample_idx in range(num_samples):
            # Forward pass for a single training sample
            input_sample = X_train[sample_idx, :].reshape(-1, 1)
            label_sample = Y_train[sample_idx, :].reshape(-1, 1)
            gradients = backward_propagation(input_sample, label_sample, activation, num_layers, params, input_dim,weight_decay)

            # Accumulate gradients over the batch
            for layer in range(num_layers + 1, 0, -1):
                grad_accu[f'W{layer}'] += gradients[f'W{layer}']
                grad_accu[f'b{layer}'] += gradients[f'b{layer}']

            num_processed += 1
            if num_processed % batch_size == 0:
                time_step += 1
                num_batches += 1

                # Compute momentum estimates
                for layer in range(1, num_layers + 2):
                    momentum[f'W{layer}'] = beta1 *prev_m[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']
                    momentum[f'b{layer}'] = beta1 *prev_m[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']

                # Bias-corrected momentum
                    m_hat[f'W{layer}'] = momentum[f'W{layer}'] / (1-np.power(beta1, time_step))
                    m_hat[f'b{layer}'] = momentum[f'b{layer}'] / (1-np.power(beta1, time_step))
                    prev_m[f'W{layer}'] = momentum[f'W{layer}']
                    prev_m[f'b{layer}'] = momentum[f'b{layer}']

                # Compute velocity estimates
                    velocity[f'W{layer}'] = beta2 * prev_v[f'W{layer}'] + (1-beta2) * (grad_accu[f'W{layer}']**2)
                    velocity[f'b{layer}'] = beta2 * prev_v[f'b{layer}'] + (1-beta2) * (grad_accu[f'b{layer}']**2)

                # Bias-corrected velocity
                    v_hat[f'W{layer}'] = velocity[f'W{layer}'] / (1-np.power(beta2, time_step))
                    v_hat[f'b{layer}'] = velocity[f'b{layer}'] / (1-np.power(beta2, time_step))
                    prev_v[f'W{layer}'] = velocity[f'W{layer}']
                    prev_v[f'b{layer}'] = velocity[f'b{layer}']

                # Update parameters using Nadam update rule
                    params[f'W{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'W{layer}'])+epsilon)) * (beta1*m_hat[f'W{layer}'] + (1-beta1)*grad_accu[f'W{layer}']/(1-np.power(beta1, time_step)))
                    params[f'b{layer}'] -= (learning_rate/(np.sqrt(v_hat[f'b{layer}'])+epsilon)) * (beta1*m_hat[f'b{layer}'] + (1-beta1)*grad_accu[f'b{layer}']/(1-np.power(beta1, time_step)))

        # Compute training accuracy after epoch completion
        train_acc = compute_accuracy(X_train, Y_train, params, activation, num_layers, input_dim)

        # Compute training loss using cross-entropy
        act, out, train_preds = forward_propagation(X_train.T, params, activation, num_layers, input_dim)
        train_loss = mse_loss(Y_train.T, train_preds, params, weight_decay, num_layers)
        train_losses.append(train_loss)  # Store loss for visualization

        # Compute test accuracy
        test_acc = compute_accuracy(X_test, Y_test, params, activation, num_layers, input_dim)

        # Compute test loss using cross-entropy
        act, out, test_preds = forward_propagation(X_test.T, params, activation, num_layers, input_dim)
        test_loss = mse_loss(Y_test.T, test_preds, params, weight_decay, num_layers)
        test_losses.append(test_loss)  # Store loss for visualization


        # Print epoch summary
        print(f"Epoch {epoch+1}, Train_loss: {train_loss:.4f}, Train_accuracy: {train_acc * 100:.2f}%, Test_loss: {test_loss:.4f}, Test_accuracy: {test_acc * 100:.2f}%")


    # Plot the loss function over epochs
    plt.plot(train_losses, label="Training Loss")
    plt.plot(test_losses, label="Test Loss")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Losses over Epochs')
    plt.show()

    return params

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=64
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=5
num_neurons=128
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)

parameters = rmsprop_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay)

parameters = adam_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

parameters = nadam_opt_mse(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

"""## Question-10 : Best three hyperparameter configurations for the MNSIT dataset"""

from keras.datasets import mnist

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Flatten the images (convert 28x28 matrices into 1D arrays of size 784)
# Normalize pixel values to the range [0,1] by dividing by 255
X_train = x_train.reshape(x_train.shape[0], -1) / 255
X_test = x_test.reshape(x_test.shape[0], -1) / 255

def one_hot_encod(arr):
    """ Convert class labels to one-hot encoded vectors.
    Args: arr (numpy array): Array of class labels (e.g., [0, 1, 2, ..., 9])

    Returns: numpy array: One-hot encoded matrix of shape (len(arr), 10)
    """
    mat = np.zeros((len(arr), 10))  # Create a matrix of zeros with shape (num_samples, num_classes)
    for i in range(len(arr)):
        mat[i, arr[i]] = 1  # Set the corresponding class index to 1
    return mat

# Convert labels to one-hot encoding for training, validation, and test sets
Y_train = one_hot_encod(y_train)
Y_test = one_hot_encod(y_test)

"""NADAM Optimizer"""

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=32
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=4
num_neurons=64
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)

# Nadam optimizer (Adam with Nesterov momentum)
parameters = nadam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

"""ADAM Optimizer

"""

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=64
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=5
num_neurons=64
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)
parameters = adam_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta1, beta2,weight_decay)

"""RMSprop Optimizer"""

# Best hyperparameters after tuning with wandb
epochs=10
batch_size=64
learning_rate=0.0001
beta=0.9
beta1=0.9
beta2=0.99
epsilon=0.000001
num_layers=5
num_neurons=128
input_dim=784
output_dim=10
weight_decay=0.0
weight_init='xavier'
activation='relu'

# Initialize the network with random weights based on the chosen weight initialization method
params = initialize_network(num_layers, num_neurons, weight_init, input_dim, output_dim)
parameters = rmsprop_opt(learning_rate, params, X_train, Y_train, X_test, Y_test, activation, epochs, num_layers, input_dim, batch_size, epsilon, beta,weight_decay)

