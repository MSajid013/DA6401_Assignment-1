{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TYJdY2YSVO_",
        "outputId": "dff19820-5bb4-465e-8154-1392a8eb74b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "nxXm3OwGIO8U",
        "outputId": "f897992d-6f27-4b63-b87c-c8ee3ea3e47b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m013\u001b[0m (\u001b[33mma23m013-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-1"
      ],
      "metadata": {
        "id": "PpB8iMFDh9tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n"
      ],
      "metadata": {
        "id": "cIcsuDPEPXYa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB project\n",
        "wandb.init(project='DA6401_Assignment-1')\n",
        "\n",
        "# Load Fashion-MNIST dataset (train and test sets)\n",
        "(train_img, train_lbl), (test_img, test_lbl) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names for labels\n",
        "class_names = ['Top/Tshirt', 'Pullover', 'Trouser', 'Shirt', 'Coat', 'Dress', 'Sandal', 'Sneaker', 'Ankle-boot', 'Bag']\n",
        "\n",
        "# Select one sample image for each class\n",
        "samples = {}\n",
        "for i in range(100):  # Loop through first 36 images\n",
        "  lbl = train_lbl[i]  # Get label of current image\n",
        "  if lbl not in samples:  # If label is not already added, add it\n",
        "    samples[lbl] = train_img[i]\n",
        "  if len(samples) == 10:  # Stop when all 10 classes have at least one image\n",
        "    break\n",
        "\n",
        "# Convert dictionary to lists for easy indexing\n",
        "sample_lbl = list(samples.keys())\n",
        "sample_img = list(samples.values())\n",
        "\n",
        "# Function to plot 7 random sample images\n",
        "def plot_sample(step):\n",
        "  fig, axes = plt.subplots(1, 7, figsize=(14, 5))  # Create figure with 7 subplots\n",
        "  chosen_indices = np.random.choice(len(sample_img), 7, replace=True)  # Randomly select 7 images\n",
        "\n",
        "  for ax, idx in zip(axes.flatten(), chosen_indices):\n",
        "    ax.imshow(sample_img[idx], cmap='gray')  # Display image\n",
        "    ax.set_title(class_names[sample_lbl[idx]])  # Set title with class name\n",
        "    ax.axis('off')  # Remove axis for cleaner view\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return fig\n",
        "\n",
        "# Log images to WandB for visualization\n",
        "wandb.log({\"Examples\": [wandb.Image(plot_sample(0), caption=\"Step 0\"),\n",
        "    wandb.Image(plot_sample(1), caption=\"Step 1\"), wandb.Image(plot_sample(2), caption=\"Step 2\")]})\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "PGnnaEUd4s7y",
        "outputId": "88122499-d36d-49a2-f60f-af54164b8387"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250308_165034-wied9zjf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1/runs/wied9zjf' target=\"_blank\">worthy-capybara-16</a></strong> to <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1/runs/wied9zjf' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1/runs/wied9zjf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 7 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABW0AAADhCAYAAACz3Ze3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAT/NJREFUeJzt3XecFdX9//HPZcvdXmALfReWJiKCC0GQsoCyIqAgoKASsGIsqA9LYn5JxMQSW9SQ2JJ8saFEDdgAARUrTQSElY70spRl2d7n9wcPri57Poe9lwVm4fV8PPzD95nPzNy5c+bOnL3c43EcxxEAAAAAAAAAgCs0ON07AAAAAAAAAAD4GYO2AAAAAAAAAOAiDNoCAAAAAAAAgIswaAsAAAAAAAAALsKgLQAAAAAAAAC4CIO2AAAAAAAAAOAiDNoCAAAAAAAAgIswaAsAAAAAAAAALsKgLQAAAAAAAAC4CIO2J2DChAkSFRV13OUyMjIkIyOjzra7detW8Xg88vTTT9fZOoEzTW37p4iIx+ORyZMnn9wdAgAAAHBKeDweueOOO4673Kuvvioej0e2bt168ncKqOcYizr1zspB2xdeeEE8Ho/06NHjdO9KvbNmzRqZPHkyH2pnKI/HU6v/vvjiizrfdlVVlSQmJsqTTz5Zq31ITU2t832ojdmzZzPAC78dfSA4+l9YWJg0bdpUMjMz5e9//7vk5+ef7l0ETjn6Bc42p/M+CziTrF69WkaNGiUpKSkSFhYmzZo1k0suuUSmTJly0rf92GOPyfvvv3/St4Oz1+k8v+E+wad7B06HadOmSWpqqixdulQ2bdokbdq0Od27VG+sWbNGHn74YcnIyDhtg2Y4ed54441q///666/L/Pnza+TnnHNOnW976dKlcuDAARkyZIg0bdq0WttNN90kv/rVr+SWW27xZbX9Fm1tFBcXS3Bw7S6Hs2fPln/+858M3CIgf/7zn6VVq1ZSXl4ue/fulS+++ELuvvtu+dvf/iYffvihdO7c+XTvInDK0S9wtjid91nAmWLhwoXSv39/admypdx8883SuHFj2bFjhyxevFief/55ufPOO/1a37hx42TMmDHi9Xprtfxjjz0mo0aNkuHDhwew94BdXZ/fqP/OukHbLVu2yMKFC2XGjBkyceJEmTZtmjz00EOne7cAV7juuuuq/f/ixYtl/vz5NfKTYfbs2ZKSkiLnnnuunHvuudXabr31VmnduvVJ24+wsLDjLlNYWCiRkZEnZfs4ewwePFi6devm+/8HH3xQPv/8cxk6dKhcfvnlsnbtWgkPDzfWcg7iTEW/wNki0PusoqIiiYiIOJm7dlLQP3EyPProoxIbGyvfffedxMXFVWvbt2+f3+sLCgqSoKAg6zKO40hJSYn6WQTUlbo+v+uj+vqZd7KcdT+PMG3aNImPj5chQ4bIqFGjZNq0adXaf/kbHa+88oqkpaWJ1+uV7t27y3fffXfc9a9cuVISExMlIyNDCgoK1OVKS0vloYcekjZt2ojX65UWLVrIAw88IKWlpX69nmeffVZSUlIkPDxc+vXrJ1lZWTWW+fzzz6VPnz4SGRkpcXFxcsUVV8jatWtrLLdixQoZPHiwxMTESFRUlAwcOFAWL17sa3/11Vdl9OjRIiLSv39//gnXWaqwsFDuvfdeadGihXi9Xmnfvr08/fTT4jhOteWO/o7UtGnTpH379hIWFibp6eny1VdfGdc7a9YsGTJkSK32oby8XB5++GFp27athIWFSaNGjaR3794yf/78Gsvu2rVLhg8fLlFRUZKYmCj33XefVFZW1tjXX35zdvLkyeLxeGTNmjVyzTXXSHx8vPTu3VsmTJgg//znP301R/8DTsSAAQPkj3/8o2zbtk3efPNNEfn5N5k3b94sl112mURHR8u1114rIkd+SuS5556Tc889V8LCwiQ5OVkmTpwohw4dqrbeZcuWSWZmpiQkJEh4eLi0atVKbrjhhmrLTJ8+XdLT0yU6OlpiYmLkvPPOk+eff/7UvHDAgn6Bs1VGRoZ06tRJvv/+e+nbt69ERETI73//exE58sB+4403SnJysoSFhcn5558vr732WrX6L774wnh/fvQZ59VXX/Vle/fuleuvv16aN28uXq9XmjRpIldccUWNn0GbM2eO71kiOjpahgwZIj/++GO1ZWz9E6hLmzdvlnPPPbfGgJaISFJSUo3s/fffl06dOonX65Vzzz1XPvnkk2rtpt+0TU1NlaFDh8rcuXOlW7duEh4eLi+//LJ4PB4pLCyU1157zfccMGHChDp+hTib1fb8PvqsfbzzW+TI8/ANN9wgycnJvuX+7//+r9oyZWVl8qc//UnS09MlNjZWIiMjpU+fPrJgwYLj7rPjOHLLLbdIaGiozJgxw5e/+eabkp6eLuHh4dKwYUMZM2aM7Nixo1qt7TMPR5x137SdNm2aXHnllRIaGipjx46VF198Ub777jvp3r17teXeeustyc/Pl4kTJ4rH45Enn3xSrrzySvnpp58kJCTEuO7vvvtOMjMzpVu3bvLBBx+of4mrqqqSyy+/XL755hu55ZZb5JxzzpHVq1fLs88+Kxs2bKj1b+S8/vrrkp+fL7fffruUlJTI888/LwMGDJDVq1dLcnKyiIh8+umnMnjwYGndurVMnjxZiouLZcqUKXLRRRfJ8uXLfT9x8OOPP0qfPn0kJiZGHnjgAQkJCZGXX35ZMjIy5Msvv5QePXpI3759ZdKkSfL3v/9dfv/73/v+6Rb/hOvs4TiOXH755bJgwQK58cYbpUuXLjJ37ly5//77ZdeuXfLss89WW/7LL7+U//73vzJp0iTxer3ywgsvyKWXXipLly6VTp06+Zbbu3evrFixQv785z/Xaj8mT54sjz/+uO9nE/Ly8mTZsmWyfPlyueSSS3zLVVZWSmZmpvTo0UOefvpp+fTTT+WZZ56RtLQ0+c1vfnPc7YwePVratm0rjz32mDiOI127dpXdu3cb/ykjcCLGjRsnv//972XevHly8803i4hIRUWFZGZmSu/eveXpp5/2/cV54sSJ8uqrr8r1118vkyZNki1btsg//vEPWbFihXz77bcSEhIi+/btk0GDBkliYqL87ne/k7i4ONm6dWu1G6n58+fL2LFjZeDAgfLEE0+IiMjatWvl22+/lbvuuuvUHwTgGPQLnK0OHjwogwcPljFjxsh1110nycnJUlxcLBkZGbJp0ya54447pFWrVvLuu+/KhAkTJDc3N6Dzc+TIkfLjjz/KnXfeKampqbJv3z6ZP3++bN++3feM8MYbb8j48eMlMzNTnnjiCSkqKpIXX3xRevfuLStWrKj2c2la/wTqUkpKiixatEiysrKqPU+YfPPNNzJjxgy57bbbJDo6Wv7+97/LyJEjZfv27dKoUSNr7fr162Xs2LEyceJEufnmm6V9+/byxhtv1PjZtrS0tDp7bUBdn9/Z2dly4YUX+gZ5ExMTZc6cOXLjjTdKXl6e3H333SIikpeXJ//+979l7NixcvPNN0t+fr785z//kczMTFm6dKl06dLFuA+VlZVyww03yH//+1+ZOXOm70tYjz76qPzxj3+Uq666Sm666SbZv3+/TJkyRfr27SsrVqyoNiht+szDLzhnkWXLljki4syfP99xHMepqqpymjdv7tx1112+ZbZs2eKIiNOoUSMnJyfHl3/wwQeOiDgfffSRLxs/frwTGRnpOI7jfPPNN05MTIwzZMgQp6SkpNp2+/Xr5/Tr18/3/2+88YbToEED5+uvv6623EsvveSIiPPtt99aX8fRfQwPD3d27tzpy5csWeKIiHPPPff4si5dujhJSUnOwYMHfdkPP/zgNGjQwPn1r3/ty4YPH+6EhoY6mzdv9mW7d+92oqOjnb59+/qyd9991xERZ8GCBdZ9xJnh9ttvd355mXj//fcdEXEeeeSRasuNGjXK8Xg8zqZNm3yZiDgi4ixbtsyXbdu2zQkLC3NGjBhRrf4///mPEx4e7hQVFRn3IzIy0hk/frzv/88//3xnyJAh1n0fP368IyLOn//852p5165dnfT09GqZiDgPPfSQ7/8feughR0ScsWPH1ljvsccEqI2pU6c6IuJ899136jKxsbFO165dHcf5+fz93e9+V22Zr7/+2hERZ9q0adXyTz75pFo+c+bM427vrrvucmJiYpyKiopAXxZwQugXONuZ7in69evniIjz0ksvVcufe+45R0ScN99805eVlZU5PXv2dKKiopy8vDzHcRxnwYIFxnv1o88PU6dOdRzHcQ4dOuSIiPPUU0+p+5efn+/ExcU5N998c7V87969TmxsbLVc659AXZs3b54TFBTkBAUFOT179nQeeOABZ+7cuU5ZWVm15UTECQ0NrfZ88sMPPzgi4kyZMsWXHf0s2rJliy9LSUlxRMT55JNPamz/2OcSoC7V9fl94403Ok2aNHEOHDhQrX7MmDFObGys7/m7oqLCKS0trbbMoUOHnOTkZOeGG27wZUc/S5566imnvLzcufrqq53w8HBn7ty5vmW2bt3qBAUFOY8++mi19a1evdoJDg6ulmufefjZWfXzCNOmTZPk5GTp37+/iBz5SvnVV18t06dPr/HPpa+++mqJj4/3/X+fPn1EROSnn36qsd4FCxZIZmamDBw4UGbMmHHcHzF/99135ZxzzpEOHTrIgQMHfP8NGDDAt77aGD58uDRr1sz3/7/61a+kR48eMnv2bBER2bNnj6xcuVImTJggDRs29C3XuXNnueSSS3zLVVZWyrx582T48OHSunVr33JNmjSRa665Rr755hvJy8ur1T7hzDZ79mwJCgqSSZMmVcvvvfdecRxH5syZUy3v2bOnpKen+/6/ZcuWcsUVV8jcuXOr9bnZs2dL//79a/07UXFxcfLjjz/Kxo0bj7vsrbfeWu3/+/TpY+zHtakFTqaoqCjJz8+vlh37jfB3331XYmNj5ZJLLqn2+ZGeni5RUVG+z4+jf73++OOPpby83Li9uLg4KSwsNP6sCOAW9Aucjbxer1x//fXVstmzZ0vjxo1l7NixviwkJEQmTZokBQUF8uWXX/q1jfDwcAkNDZUvvviixs+IHDV//nzJzc2VsWPHVutbQUFB0qNHD+MzS23+JRNwIi655BJZtGiRXH755fLDDz/Ik08+KZmZmdKsWTP58MMPqy178cUXV/smbOfOnSUmJqZWzwKtWrWSzMzMOt9/wKYuz2/HceR///ufDBs2TBzHqXYdz8zMlMOHD8vy5ctF5MhvO4eGhorIkX8ZnpOTIxUVFdKtWzffMr9UVlYmo0ePlo8//lhmz54tgwYN8rXNmDFDqqqq5Kqrrqq2zcaNG0vbtm1rfHaYPvPws7Nm0LayslKmT58u/fv3ly1btsimTZtk06ZN0qNHD8nOzpbPPvus2vItW7as9v9HB3CPvakpKSmRIUOGSNeuXeWdd97xneg2GzdulB9//FESExOr/deuXTsR+fkHpnNycmTv3r2+/w4fPlxtPW3btq2x7nbt2vl+j2fbtm0iItK+ffsay51zzjly4MABKSwslP3790tRUZG6XFVVVY3fHsHZadu2bdK0aVOJjo6ulh/9iYyj59xR2jlaVFQk+/fvF5Ejv087f/78Wv+erciRmcZzc3OlXbt2ct5558n9998vq1atqrFcWFiYJCYmVsvi4+PVh5NjtWrVqtb7BJyogoKCan0rODhYmjdvXm2ZjRs3yuHDhyUpKanGZ0hBQYHv86Nfv34ycuRIefjhhyUhIUGuuOIKmTp1arXfTb/tttukXbt2MnjwYGnevLnccMMNxt/BAk4n+gXORs2aNavxTLFt2zZp27atNGhQ/fFNuwc7Hq/XK0888YTMmTNHkpOTpW/fvvLkk0/K3r17fcsc/eP4gAEDavStefPm1ZgUx9Q/gZOhe/fuMmPGDDl06JAsXbpUHnzwQcnPz5dRo0bJmjVrfMsd+0wvUvtnAZ4DcLrU1fm9f/9+yc3NlVdeeaXGNfzoIOkvr+OvvfaadO7c2TdnTGJiosyaNavGOJSIyOOPPy7vv/++vPfee5KRkVGtbePGjeI4jrRt27bGdteuXVvjs8P0mYefnTW/afv555/Lnj17ZPr06TJ9+vQa7dOmTav21wFtBknnmMmWvF6vXHbZZfLBBx/IJ598IkOHDj3uvlRVVcl5550nf/vb34ztLVq0EBGRK6+8stpfzcePH19t8gDgTHD0m9yXXXZZrWv69u0rmzdvlg8++EDmzZsn//73v+XZZ5+Vl156SW666SbfcsebCfZ4mCEWp8rOnTvl8OHD0qZNG1/m9XprPJxXVVVJUlJSjUk0jzr6RwqPxyPvvfeeLF68WD766COZO3eu3HDDDfLMM8/I4sWLJSoqSpKSkmTlypUyd+5cmTNnjsyZM0emTp0qv/71r2tMbAOcDvQLnK1O5P5DmyD12H9VKCJy9913y7Bhw+T999+XuXPnyh//+Ed5/PHH5fPPP5euXbtKVVWViBz5XdvGjRvXqA8Orv4oaeqfwMkUGhoq3bt3l+7du0u7du3k+uuvl3fffVceeughEan9M70JzwE43U70/D56Db/uuutk/PjxxmU7d+4sIkcmDZswYYIMHz5c7r//fklKSpKgoCB5/PHHZfPmzTXqMjMz5ZNPPpEnn3xSMjIyJCwszNdWVVUlHo9H5syZY9zHqKioav9PX7M7awZtp02bJklJSb6Z339pxowZMnPmTHnppZf8Xq/H45Fp06bJFVdcIaNHj5Y5c+bU+EvDsdLS0uSHH36QgQMHWmeef+aZZ6r9FbBp06bV2k3/NHzDhg2+CQFSUlJE5MiPqB9r3bp1kpCQIJGRkRIWFiYRERHqcg0aNPANJNv2F2e+lJQU+fTTTyU/P7/aN5/WrVvna/8l7RyNiIjwPUTPmjVLOnbsWG0ii9po2LChXH/99XL99ddLQUGB9O3bVyZPnlxt0PZkoA/gZDg6sd3x/hleWlqafPrpp3LRRRfV6gbnwgsvlAsvvFAeffRReeutt+Taa6+V6dOn+/pJaGioDBs2TIYNGyZVVVVy2223ycsvvyx//OMfqw2UAacD/QL4WUpKiqxatUqqqqqqDYweew929F8H5ubmVqvXvomblpYm9957r9x7772yceNG6dKlizzzzDPy5ptv+v7ZbVJSklx88cV1/ZKAOtWtWzcROfITgScTzwI4HQI5vxMTEyU6OloqKyuPew1/7733pHXr1jJjxoxq5/jRAeJjXXjhhXLrrbfK0KFDZfTo0TJz5kzfH/LS0tLEcRxp1aqV71+TI3BnxZ9Ci4uLZcaMGTJ06FAZNWpUjf/uuOMOyc/Pr/EbIbUVGhoqM2bMkO7du8uwYcNk6dKl1uWvuuoq2bVrl/zrX/8y7mthYaGIiKSnp8vFF1/s+69jx47Vln3//fdl165dvv9funSpLFmyRAYPHiwiR36TtkuXLvLaa69Vu3HLysqSefPm+b7ZGBQUJIMGDZIPPvjA99MKIkdmGnzrrbekd+/eEhMTIyIikZGRIlLzRhBnh8suu0wqKyvlH//4R7X82WefFY/H4zv3jlq0aFG138DZsWOHfPDBBzJo0CDfX91mz57t108jiByZYfKXoqKipE2bNtX+ievJQh9AXfv888/lL3/5i7Rq1UquvfZa67JXXXWVVFZWyl/+8pcabRUVFb7z8tChQzW+RXJ01tej/eTYftSgQQPfX9tPRV8CbOgXQHWXXXaZ7N27V/773//6soqKCpkyZYpERUVJv379ROTI4G1QUJB89dVX1epfeOGFav9fVFQkJSUl1bK0tDSJjo72neuZmZkSExMjjz32mPF3oI/+1BVwKi1YsMD4Tdmj87WYfvKvLkVGRvIcgJOmLs/voKAgGTlypPzvf/+TrKysGu2/vIYffTb/5baXLFkiixYtUtd/8cUXy/Tp0+WTTz6RcePG+b7Ze+WVV0pQUJA8/PDDNV6L4zg17rVgd1Z80/bDDz+U/Px8ufzyy43tF154oSQmJsq0adOkR48eAW0jPDxcPv74YxkwYIAMHjxYvvzyS+nUqZNx2XHjxsk777wjt956qyxYsEAuuugiqayslHXr1sk777wjc+fO9f0lxaZNmzbSu3dv+c1vfiOlpaXy3HPPSaNGjeSBBx7wLfPUU0/J4MGDpWfPnnLjjTdKcXGxTJkyRWJjY2Xy5Mm+5R555BGZP3++9O7dW2677TYJDg6Wl19+WUpLS+XJJ5/0LdelSxcJCgqSJ554Qg4fPixer1cGDBggSUlJAR031C/Dhg2T/v37y//7f/9Ptm7dKueff77MmzdPPvjgA7n77rur/RC6iEinTp0kMzNTJk2aJF6v1/fA8PDDD4uIyJYtW2Tt2rXy4osv+rUfHTt2lIyMDElPT5eGDRvKsmXL5L333pM77rijbl6oxdGJ1SZNmiSZmZkSFBQkY8aMOenbxZlhzpw5sm7dOqmoqJDs7Gz5/PPPZf78+ZKSkiIffvhhtX9aZNKvXz+ZOHGiPP7447Jy5UoZNGiQhISEyMaNG+Xdd9+V559/XkaNGiWvvfaavPDCCzJixAhJS0uT/Px8+de//iUxMTG+P9jddNNNkpOTIwMGDJDmzZvLtm3bZMqUKdKlSxffbyQCpwL9Aji+W265RV5++WWZMGGCfP/995KamirvvfeefPvtt/Lcc8/5/gVUbGysjB49WqZMmSIej0fS0tLk448/rvEbghs2bJCBAwfKVVddJR07dpTg4GCZOXOmZGdn++5rYmJi5MUXX5Rx48bJBRdcIGPGjJHExETZvn27zJo1Sy666KIaf8gHTrY777xTioqKZMSIEdKhQwcpKyuThQsXyn//+19JTU096RMapaeny6effip/+9vfpGnTptKqVauAxxCAY9X1+f3Xv/5VFixYID169JCbb75ZOnbsKDk5ObJ8+XL59NNPJScnR0REhg4dKjNmzJARI0bIkCFDZMuWLfLSSy9Jx44dpaCgQF3/8OHDfT8jFRMTIy+//LKkpaXJI488Ig8++KBs3bpVhg8fLtHR0bJlyxaZOXOm3HLLLXLfffed0HE6qzhngWHDhjlhYWFOYWGhusyECROckJAQZ9myZY6IOE899VSNZUTEeeihh3z/P378eCcyMrLaMgcOHHA6duzoNG7c2Nm4caPjOI7Tr18/p1+/ftWWKysrc5544gnn3HPPdbxerxMfH++kp6c7Dz/8sHP48GHr69myZYtvH5955hmnRYsWjtfrdfr06eP88MMPNZb/9NNPnYsuusgJDw93YmJinGHDhjlr1qypsdzy5cudzMxMJyoqyomIiHD69+/vLFy4sMZy//rXv5zWrVs7QUFBjog4CxYssO4v6q/bb7/dOfYykZ+f79xzzz1O06ZNnZCQEKdt27bOU0895VRVVVVbTkSc22+/3XnzzTedtm3bOl6v1+natWu18+Uf//iHExsb65SXl1v3IzIy0hk/frzv/x955BHnV7/6lRMXF+eEh4c7HTp0cB599FGnrKzMt4ypfzqO4zz00EM1XtOxffvoMvv3769RX1FR4dx5551OYmKi4/F4aqwLMJk6daojIr7/QkNDncaNGzuXXHKJ8/zzzzt5eXnVltfO36NeeeUVJz093QkPD3eio6Od8847z3nggQec3bt3O45z5Ho+duxYp2XLlo7X63WSkpKcoUOHOsuWLfOt47333nMGDRrkJCUlOaGhoU7Lli2diRMnOnv27Dk5BwE4Bv0CZzvTfVa/fv2cc88917h8dna2c/311zsJCQlOaGioc9555zlTp06tsdz+/fudkSNHOhEREU58fLwzceJEJysryxER3/IHDhxwbr/9dqdDhw5OZGSkExsb6/To0cN55513aqxvwYIFTmZmphMbG+uEhYU5aWlpzoQJE6r1neP1T6CuzJkzx7nhhhucDh06OFFRUU5oaKjTpk0b584773Sys7N9yx19FjlWSkpKteeKo59FW7ZsqbbMkCFDjNtft26d07dvXyc8PNwRkWrrAk5UXZ/fjnPks+P22293WrRo4YSEhDiNGzd2Bg4c6Lzyyiu+ZaqqqpzHHnvMSUlJ8T23f/zxx8748eOdlJQU33K/HIv6pRdeeMEREee+++7zZf/73/+c3r17O5GRkU5kZKTToUMH5/bbb3fWr1/vW8b2mYcjPI5Ti1/hBgA/eTweuf32263fwLjsssskKipK3nnnnVO4ZwAAAAAAAO52Vvw8AgB3ysjIkD59+pzu3QAAAAAAAHAVBm0BnDa//P1lAAAAAAAAHNHgdO8AAAAAAAAAAOBnfNMWwEnBz2UDAAAAAAAEhm/aAgAAAAAAAICLMGgLAAAAAAAAAC7CoC0AAAAAAAAAuEitf9PW4/GczP0ATpsT+e1VN/QLbR9O1W/KdujQQW37xz/+YczfffddtWbFihXGvKysTK0pLy9X2zp16mTMR4wYodZs3rzZmD/11FNqTW5urtpWH9X3fgGcDPQLoKZA+8Wp6hO27Zyqe6WkpCRjPmDAALXmpptuMua2+421a9cac9s9VFxcnNrWq1cvY7548WK15ve//70xLy4uVmsCcbrvf234rPhZamqq2paRkWHMr7jiCrXm4MGDxvzNN99Ua5YvX27Mbc8wI0eOVNsGDhxozIuKitQabf9eeeUVteZMQ78AaqpNv+CbtgAAAAAAAADgIgzaAgAAAAAAAICLMGgLAAAAAAAAAC7CoC0AAAAAAAAAuAiDtgAAAAAAAADgIgzaAgAAAAAAAICLeBzHcWq1oMdzsvcFOC1q2QWM6rpfaOs7kX006dKlizEfM2aMWjNy5EhjXllZqdZERkYa8/DwcLWmUaNGaltd2rBhg9pWVVVlzNu3b6/WZGdnG/O5c+eqNU8//bQxz8rKUmtOFTf1C8At6BdATYH2CzffQyUkJKhtd911lzG/+OKL1Rqv12vMCwsL/a7p0KGDWhMdHa22acrLy9W2nTt3GvM9e/aoNdo9Xk5Ojlrz1VdfGfMpU6aoNYcOHVLbTrcz9bNi8ODBats999xjzIuLi9Wa0NBQY15SUqLWaOd4p06d1Jrk5GRjvnXrVrWmoqJCbdPO/8OHD6s1Wn9u1qyZWvPZZ58Z80mTJqk1bnam9gvgRNSmX/BNWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEY9Ty2n8mLEPZ6r6PpNlTEyMMX/99dfVms6dOxvzBg30v+Pk5+cbc9sMr9qMxJWVlWpNSEiIMY+NjVVrbLMvV1VVGfMTed9NwsLCjLk2i7KIPmvu119/rdaMGzfOvx0LUH3vF/WR7bhpfVM7v0UCew8Dee/qui9pevXqpbYtXLjQmLdv316t2bBhgzG3vR76xalHv7Crz/2irvuEtj7b/qWlpRnzjz76SK3Jzs425nV9P1RaWmrMc3Jy1JqoqKg6246Ifp+SmJio1gQHB/u1LltbUVGRWvPSSy8Z85kzZ6o1p0p9/6zQ+sXkyZPVGq1fREREqDWBXMMrKiqMeYsWLdQajW07trbDhw8bc23fRPRrgK0/N2vWzJjn5uaqNffdd5/adrrV934BnAy16Rd80xYAAAAAAAAAXIRBWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEY/jOE5tFmzQQB/freUq6g2Px2PMba8zkJqgoCBjXlVVpdbU9T6cqho3O5H91o7FqfTpp58a85SUFLXm4MGDxtx27gUHBxvziooKtSaQ46Nda8rKytQarS8Fsp26ZjsG2rnXpEkTtSYzM9OYr1u3zr8dO4763i/qI9tx087XysrKk7U7J1VGRobadt555xnztm3bqjWdO3c25rZjOmjQIGNeWlqq1tAvTj36xRFnYr9wQ5945513jHlCQoJak5OTY8xDQkLUGu0YlZeXqzXaPZntvdDaSkpK1Bqv16u2xcbGGnPba63Le7/Q0FC1RtuH4cOHqzUFBQV+7Veg6vtnxQsvvGDMbeeRdr5GRUWpNWFhYcbc9mxRVFTkd83hw4f92r6I/ZnI1mc02ueSbb+1492pUye15vXXXzfms2bNsuzdqVHf+wVwMtSmX/BNWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEfN08AaBzPZnm9lQmynRNsPksmXL/N6HQATyWgOpqevZjU/Vfp/IzI8ITHp6utqWkpJizA8cOKDWBAebu35QUJBao82w2qxZM7UmIiLCmGuzBIvoMylr+yxi70vabKO2mY+161N+fr5as3PnTr/WZWN7PTfddJMxv++++/zeDk6cdn7V9bW1rj8vfv3rXxvzxYsXqzV9+vQx5pMmTVJrdu/ebcy1We1FRDZu3GjMly9frtbcfffdxnzlypVqDU4e+gX9wk2aNGmitjVu3NiYa7PNi4iEhoYac9vnvXY/FBkZqdZo90q2We21PmHrK9r9nYi+f7b1acfBVlNQUGDMS0pK/N63YcOGqTVvv/222oafvfrqq8b8nnvuUWv2799vzLOzs9Wa6OhoY649C9iUlZWpbQkJCX6vLy8vT20rLi72e30a237HxsYa8x07dqg1s2bNOuF9AuAufNMWAAAAAAAAAFyEQVsAAAAAAAAAcBEGbQEAAAAAAADARRi0BQAAAAAAAAAXYdAWAAAAAAAAAFyEQVsAAAAAAAAAcJHg2i4YERGhtl111VXG/PLLL1drVq1aZcyrqqrUmj59+hjzHTt2qDVxcXHGPDo6Wq3ZtGmTMU9ISFBrDhw4oLZptH0rLS1Va2zHJygoyJjb9js3N9evdYnY90/jOI4xDwkJUWu0Nq/Xq9Zor3Xq1KmWvXO//v37q23a8bAdJ+08CuR9/+1vf6vW7N6925jv3LlTrWnatKkx37Nnj1rToIH+96eysjJjbjs+UVFRxvyCCy5Qa+68805jbrs2BAebL8G2fj5q1Chjft9996k1OHN16NBBbdPOLxGRjIwMY96tWze1Jj4+3pi/+uqras1XX31lzJcvX67WpKenG/Pu3burNVo/b9OmjVqjfdaj/qNfHEG/OEJ7j0REGjdubMwrKyvVmtDQUGMeGRmp1lRUVBjzQO7VPB6PWmNr09ju/bT12e5TtBrbMU1MTDTmtnso7X245JJL1Jq3335bbcPPli5daswXLVqk1mjP/UuWLFFrtOuxbdzh4MGDxly73ono51FJSYlaY9sHbb/z8vLUGu0ct9H24Xe/+53f6wJQf/FNWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEX0K3WMMGzZMbevSpYsx/8Mf/qDW9OnTx5hfeumlao02w+PKlSvVmlatWhnz8vJytebCCy805rYZTLXZZxs1aqTWFBcXG/P9+/erNe3bt1fbcnJy/F5f586d/do3EZHc3FxjXlpaqtb07dvXmNuOj/a+rl27Vq2Jiooy5m3btlVr6oNRo0apbdqMxLbZgLUZfMPCwtSaw4cPG/N//etfas2gQYOM+QUXXKDWTJ061ZhPnDhRrcnKylLbGjZsaMxtxyc7O9uYP/vss2rNbbfdZsxtM5Vrx7uoqEit0WZFb9eunVqzYcMGtQ0nxnGcOluXbabiXr16GfO9e/eqNbZZjP/zn/8Y83vuuUet2b17tzG39YukpCRjbjtu69evN+bp6elqjTZbuG1m6E2bNqltODH0C/qFm2j3uiL6vYB2Xy8i0qCB+TsvWi6iH3Pt/BER2bx5szHfunWrWlNYWOjX9m01IvrzUmhoqFqjHe+hQ4eqNdr+xcXFqTXaPX9kZKRagxPz97//XW276667jPn27dvVGu0Z1XZOavfI+fn5ao3G9ixg2wft3j4kJESt0fYvNjZWrZkzZ44xt32OATjz8E1bAAAAAAAAAHARBm0BAAAAAAAAwEUYtAUAAAAAAAAAF2HQFgAAAAAAAABchEFbAAAAAAAAAHARBm0BAAAAAAAAwEWCa7vgrl271LaKigpj3q1bN7Wme/fuxvzw4cNqjdbWr18/tebLL7805k2bNlVrxo0bZ8w/+eQTtSY1NdWYV1VVqTXTp0835klJSWpNZGSk2taoUSNjHh4ertacc845xnzRokVqzcGDB415u3bt1Jr4+HhjXl5ertbk5eUZc9vx6d27tzGfOnWqWlMfnH/++Wrbjh07jHmDBvrfZLxer9/7EBMT43eN1mcKCwvVmo4dOxrz++67T62ZOXOm2jZs2DBjHhysX/6WL19uzNPT09Ua7Tpo67OVlZXG3Hbd2L59uzHv2bOnWrNhwwa1DScmKCjImNveQ8dxjHlUVJRaU1JSYsw7deqk1mRkZKhtEydONOaXXnqpWjN37ly1TbNv3z6/a7RrfE5OjlrTrFkzY37DDTeoNd9++60xz8rKsuwdaoN+YUe/OLW0+20Rka+//tqYX3vttWqNdn499thjas26devUNn9FRESobdo9v+1ZwHafEhYWZsxt93Fvv/22MX/wwQfVmu+++86YJycnqzVFRUXGvHXr1moNake7R9budUX0Z7BHH33U7+1r761tH2zneHFxsTG3PQvY2kpLS4257dlLY6v56KOP/F4fgDMP37QFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABfRp0U8RocOHdS25s2bG/OWLVuqNdostGlpaWpNamqqMe/cubNas2DBAmPepEkTtWbz5s3GPCEhQa3RZlHdtm2bWqMpKytT23bs2KG2nXPOOcZce39E7DPQarKzs435sGHD/K6xzUbcpk0bY96tWze1JiYmxpjbZhR1E21G4v3796s12iyq2uzdIiIej8eY247TwYMH1TaN9nq0WVdF9L5pm31Wez0iIuXl5X7X9OzZU23T7N6925hrs3eLiFRWVhpz2wzr2gy4ffr0UWtee+01tQ0nRnuvHMfxe13aeyuizy48YMAAtebNN99U22699dba79gp1qhRI2OuXd9FRJYtW2bMbdcar9fr1/ZRe/SLuke/CNyTTz6ptmnnqvb8ICKyYsUKY257L9atW2fMbfcieXl5xtx2P5abm2vMtXshEXu/1PYvNjZWrTn33HONufZ8JSJy7bXXGvOCggK1RjsOtvMbtaM9W9js2bPHmNve91atWhnzkpIStSY/P9+Y2+6dtfVpnyEi9nMvMTHRmNuOm7atQMYKcGazfS5otOu4bTwgkHu14GB9+DCQ64bG1jdtfb0uhYSEGHPb6wzkPrc2+KYtAAAAAAAAALgIg7YAAAAAAAAA4CIM2gIAAAAAAACAizBoCwAAAAAAAAAuwqAtAAAAAAAAALgIg7YAAAAAAAAA4CLBtV3w4MGDaltiYqIx37t3r1qTlpZmzBs00MeRte1kZ2erNa1btzbmV1xxhVrz/fffG/PmzZurNatWrTLmAwYMUGtatWplzLOystSa7t27q20LFy405v369VNrcnNzjfkFF1yg1lRWVhpz23uXmppqzLX3VEQkPDzcmGv7bNuHkJAQtcZNfvvb3xpz7ViIiBQUFBhz7X2yra+kpEStqaioMObdunVTaxo1amTMGzZsqNZo71VycrJaU15errZpryk0NFStiYuLM+ZXX321WhMfH2/Mi4uL1ZrY2Fi/a7T9tr0POHkcx6mzdeXn56ttX331lV/58QRyDQjktXo8Hr/X1aRJE2Oek5Oj1mjHbs6cOWpN06ZNjXlKSopag9qhX9jRL06tuXPnqm0DBw405iNHjlRrBg0aZMxfe+01teY3v/mNMdfuN0RE2rRpY8yjoqLUGu0cCgoKUmts90NlZWXGvKqqSq158803jbmtL2v3v9r2RUQOHTpkzK+88kq1plevXsbc1o9wYmzPh9HR0cbcdn55vV5jnpeXp9Zo57jt2m479zTas5LNvn37/K7Bma0u76G0+41AtxPIOW6jfTb+4Q9/UGuaNWtWp/ugsY0vnGp80xYAAAAAAAAAXIRBWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXCS4tgvaZirdsmWLMf/mm2/UmksvvdSYazP3ioisW7fOmNtmi2zcuLExf/7559Wa/v37G/PExES1Rpt91nYMtDbbjHizZ89W2zp37mzMzznnHLVm+vTpxvyTTz5Ra1JTU435qlWr1JoLL7zQmDds2FCt0axZs0Zt086R7Oxsv7dzOixcuNCYa+exiD67cExMjFoTGRlpzDdu3KjWVFZWGvPFixerNdrsr7ZZYbXt2GY+Dg7WL2XarJnadkT0mW5tMx9v2LDBmEdERKg12muyzbS7e/duY/7++++rNaj/tHPF1pds51EgNbY+U5e0z9qCggK1RuvntuuGdl9T1zPj4uShX9AvauOvf/2r2qbNDq191oqIrF271pgPGzZMrfnTn/6ktmm0fSstLVVrtPPRNku47b3VzpWQkBC1RjuHDh06pNYsXbrUmO/du1etWbBggTG33cvm5OSobTg+27VQu+7u3LlTrdGeXW3b0c5/2zmuna+263dYWJjaVlxcbMxLSkrUmoSEBGO+a9cutUZje+6pz9dq2Gmf6SL6+V/X58PYsWPVtq5duxrz0aNHqzVaXzpw4IBa8/bbb/u9b4EIDQ015g888IBa88gjj9TpPhzFN20BAAAAAAAAwEUYtAUAAAAAAAAAF2HQFgAAAAAAAABchEFbAAAAAAAAAHARBm0BAAAAAAAAwEUYtAUAAAAAAAAAFwmu7YLJyclqW05OjjHv0qWLWhMTE2PMy8vL/a6x7dv5559vzD/77DO1pqKiwpi3b99erbn33nuNeVFRkVpz3XXXGfPmzZurNVOnTlXbvvzyS2Pev39/tWb9+vXGPDw8XK0ZNWqUMY+Li1NrNm7caMy9Xq9a06xZM7/3bc2aNcY8OjparXGTF1980a9cRCQ+Pt6Yt23bVq35zW9+Y8z79eun1mj9PCsrS63Jzc015iEhIWpNUFCQ2laXPB6P2taggfnvWSUlJWpNbGysMV+1apVac+2116ptwC9VVlbWaU1xcbExD6T/2fqS4zh+ry8yMtKYjx8/Xq35+OOPjflbb72l1hQUFBhz2+c23IV+Qb+ojRkzZqhtAwcONObdunVTa+bMmWPMP/zwQ7UmKSnJmG/fvl2t0c472z1UWFiYMQ8OrvUjXzXaM5HtfCgrKzPm2nOciEhKSooxv/vuu/2uycjIUGtWrFhhzFeuXKnW4MRs3bpVbdPut0NDQ9Ua7bnHth3tPG7UqJFac+jQIb/XV1paqtZor1VbF85sgdwjBHLv0KZNG7Vt9OjRxrxXr15qzaBBg9S2zZs3G/OdO3eqNXl5ecY8NTVVrbnsssvUtro0ZswYY96jR49Tsv1f4pu2AAAAAAAAAOAiDNoCAAAAAAAAgIswaAsAAAAAAAAALsKgLQAAAAAAAAC4CIO2AAAAAAAAAOAitZ5K9Pvvv1fbhg8fbsw3bdqk1uzZs8eY22avT0xMNObPP/+8WpOcnGzMH3jgAbVGm/nx/vvvV2uys7ON+V133aXWaDNWlpeXqzU9e/ZU27RZa6dMmaLWaDOsNm7cWK354YcfjPn69evVmqFDhxrzli1bqjVZWVnG3DZr7vnnn2/MFy1apNbUd9rspkuXLlVrtHN8wIABao02Y6VthldttmvbTNxVVVVqm8Y2A6fWZtuO1+s15tqMyCL6jM0LFy5Ua+Au2rkSyGyt9VVlZaXaZuu3gaxPc+DAAWOuzfotos/0/vLLL6s1aWlpxpw+Wx39gn4hUr/7RceOHdW24uJiY7537161ZvHixcb8oosuUms6depkzG39KJBzS7u3sW0nkHuoQO7jbMf0rbfeMuYrV65Ua3766SdjvmPHDrVmw4YNahtODq2PiQR2z6/V2M5J7R7dtn3t+UpEJCEhwZhHR0erNRrbcy1OngYNzN9dtJ0T2jOv7flQE8g9VFxcnNr26KOPGvOrr75arSkqKjLm2jidiH18QTuXw8PD1Zp169YZ8+bNm6s1f/nLX9Q2TVJSkjG3HZ+//e1vxrxDhw5qTXp6ujG3jaXWBt+0BQAAAAAAAAAXYdAWAAAAAAAAAFyEQVsAAAAAAAAAcBEGbQEAAAAAAADARRi0BQAAAAAAAAAXYdAWAAAAAAAAAFwkuLYLFhYWqm2DBw825j/++KNa8/bbbxvzRo0aqTUNGzY05jt27FBrrrnmGmMeExOj1rRs2dKYL1myRK3ZvHmzMX/jjTfUmiuvvNKYN2igj6UvX75cbWvdurUx93q9ak18fLwxr6qqUmu092jFihVqjfbeadsXEZkzZ44xnzBhgloTHh5uzD0ej1pTH9j2PyQkxJiXlZWpNY7jGPO8vDy1JigoyJhXVlb6vR0b7bUGsq66ph0Dm9zc3DrdjtY33XB86juOoZ2tr/urS5cuatsPP/xgzKdPn67WDB061JhnZmaqNaGhocbcdk9xNqJf2NEv3E+7PxYRCQ42Pwo1b95crdm7d68xLyoqUmsqKiqMeX5+vlqjPQ9o6xIJ7F4tEJGRkWpbeXm5MU9MTFRrtGMXHR2t1mjvUVxcnFrTuHFjY/7TTz+pNfiZ7flQYztf9+/fb8xtzzCHDh3yex+0Gtt2tGdKEZF9+/YZc9s5XlBQoLbh5LA9PwcyNmA7X/w1cOBAtW3kyJHGXBvXEhE5ePCgMV+zZo1ao/VN2ziZbayuuLjYmNs+G7t162bMtc9ZEf043H///X7v2+rVq9UabQwtLCxMrbF9pp8IvmkLAAAAAAAAAC7CoC0AAAAAAAAAuAiDtgAAAAAAAADgIgzaAgAAAAAAAICLMGgLAAAAAAAAAC5injLVoH379mrb8uXLjbltptKOHTsa86+//lqt0WZ4veiii9SaVatWGfO8vDy15pxzzjHm27dvV2uuvfZaY247bh9//LExt83I2rt3b7VNm6115cqVao02k542m6eIPgPgkCFD1JoNGzYY8+eee06tadeunTHXzgMRfVbTFi1aqDX1gW32bu19t9m8ebMxt/UL7bgHMpOm7fVos3kGOoN5Xc4OGhIS4ve6bMdUo80YLVL3M0ADv6TNPC4S2Ln329/+1pg3bNhQrXnxxReN+bhx49Qabdbc2bNnqzUpKSnGvC5nB8aZgX5Rv/uF7TO1pKTEmNveV2126IiICLVGuz+1nVtam+2+RnuttmNgW5+237b1hYaGGnPbaz1w4IDaptH6i+05oWnTpsb8p59+8nv7ZyPb+66dK9HR0WpNfHy8MbfNNm+7Tmq088vWZ2NjY9W2QK6HWj/Trrk2FRUVftecjWzPjnX5PDVp0iS17dZbbzXmycnJas3OnTuN+erVq9Ua7fXYtqPR+rKI/Zhq1wfb+rQxp5iYGLVGs3DhQrVtxIgRfq/vD3/4gzG/7bbb1BptvPC6667ze/u/xDdtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEQZtAQAAAAAAAMBFGLQFAAAAAAAAABcJru2CGzduVNvCw8ON+d69e9Wa9evXG/Nx48apNWvWrDHma9euVWv+8Ic/GPNFixapNY0bNzbml112mVqTmJhozFu2bKnWREVFGfOSkhK15pprrlHbPvzwQ2OenJys1rRo0cKY5+fnqzVNmjTxa/siIl6v15iPGDFCrVmyZIkx//7779WaK664wphv2LBBranvGjQw/+2lsrJSrSkuLjbmZWVlao32HlZUVKg1wcHmS4zH41FrHMfxu8bWph0fbTsiIqWlpcY8IiLC732wHR/AbWzXjdTUVGM+efJktSYoKMiY79+/X60ZNWqUMbfdh2jXmqZNm6o15eXlahvwS/SL+k27DxDRP7urqqrUmpycHGOuPQ/Z1mfbN9t9ir81tnXZ7qG080G7JxTRzzvba9WeGW3PRFq/1PqXiEh0dLTahuOz9QuN7bqWlZVlzHfs2KHWaPfitnNFexa2Pfds3bpVbdO2FRsbq9bs2bPHmNuux6idCy64wJhfcsklak379u2NeVhYmFqjvVfamI6ISG5urjHftWuXWqOdR7Z909ps1/6ioiJjHhISotbYPi+0a7L2mSCiX1O0sQoRvf/96le/Umt2795tzG3v3c6dO4257b5Luz7dfPPNak1t8E1bAAAAAAAAAHARBm0BAAAAAAAAwEUYtAUAAAAAAAAAF2HQFgAAAAAAAABchEFbAAAAAAAAAHARBm0BAAAAAAAAwEWCa71gsL7o119/bcy9Xq9a079/f2Oenp6u1uzevduYl5SUqDU//fSTMW/fvr1ao3EcR237/PPPjXl0dLRak5iYaMxLS0vVmqysLLVt6dKlxjw8PFyt0d4j23sXFBRkzHfs2KHWtG3b1piPGDFCrdGOz4wZM9Sajz76yK91nQls56WmqqrKmFdWVvq9Hdv2GzTw/+9C2r5p593xeDweY27bN+01aftmW18g708gNWcj2zlhO5frI+21aue3iEhoaKjaVlRUZMw7dOig1jz11FPGfOPGjWpNixYtjPm9996r1gRy/nfp0sWYt27dWq1ZtGiR39upD+gX9Iuj6BfHp51Dts/77OxsY2673w6Edh7b9k17XrPd89jatH2wXUsCuV8rKyvzu0bb77reN5yYPn36qG3ac/q2bdvUGu25Py8vT62JiYkx5rGxsWpNcXGx2qadr02aNFFrNI0bN1bbkpKSjPm+ffvUGq1f2K4b9cEdd9yhtl155ZXG3HZN1q5ttmtRSEiIMdfuHWzbiYqKUmu096qwsFCtyc3NNea2MTxtO2FhYWqN7f5KGz+yXXe198i2D9r7YLsGVFRUGPNDhw75XWM7r2xjfyeCb9oCAAAAAAAAgIswaAsAAAAAAAAALsKgLQAAAAAAAAC4CIO2AAAAAAAAAOAiDNoCAAAAAAAAgIvo08kdo2nTpmqbNlNbZGSkWqPNcJeVleX3dsaNG6fWJCcnG/ODBw+qNdpskb169VJrtNnllixZotZoMwvbZqSbMmWK2paenm7MGzVqpNasXLnSmCcmJqo1qampxnzAgAFqzZw5c4z5999/r9bExcUZc9sMhDt27DDmtpkO8bNmzZqpbdrsirb3Q5vtOpCZik8lbf/Ky8vVGm2/man45LHNEK0J5PwKZNb2uqa9Vtv5ZZvNVuvrttnrP//8c2N+4YUXqjWjR49W2+qS9h4FenzqM/oF/eIo+sURgZyrtj6h3Q9pM1rb9sE2o7u2D9ozh4h+/2I7BoEcn0D2wXZMtWcf7XlRxD67eF3WnI2099B2vrZo0cKYd+zYUa356aefjLn2DCgikpCQYMw3bdqk1mhjEq1atVJrbOdeTEyM2uavgoICte2aa64x5s8995xaY3uP6rM33nhDbfvuu++MuW3splOnTsY8JSVFrYmOjjbm8fHxak1wsHm4zXavpvU/2/iM1mY7H7R7gdDQULVGez0i9ud7jXb+FxYWqjVlZWXG3Pa5pL2mkpISv2ts+1ZaWmrMZ82apdY88MADattRfNMWAAAAAAAAAFyEQVsAAAAAAAAAcBEGbQEAAAAAAADARRi0BQAAAAAAAAAXYdAWAAAAAAAAAFyEQVsAAAAAAAAAcJHg2i6Yl5entjVr1syYN2nSRK1ZtmyZMd+9e7dak5aWZsz37Nmj1mzdutWYp6amqjWlpaXG/IsvvlBrQkNDjfn69evVmoYNGxrznJwctSY5OVltCwkJMeYHDx5Ua1JSUvyuyc7ONuZxcXFqzUUXXWTMbcdn9uzZxrx9+/ZqTaNGjYy57Ryp7xzHqbN1VVRU+F2jnfsiIpWVlcbc4/GoNVqbrcZ2DLS6qqoqtUbrS9q1wbYP2rps6vI9RXVuPraBnONaHzueyZMnG3PbZ/D5559vzK+++uqA9qEuacchISFBrSkrKztZu1Pv0C+OoF/QL05EWFiY2qbdc9jO7wYNzN+tsdVoAu3jWp1tfdo5pL0eEZHw8HBjvmnTJrWmS5cufm1fJLBjdzay3SNrMjMzjfmaNWvUGq3P2MYdtGf4Xbt2qTUdOnQw5rbXuXPnTrWtc+fOxlx7RhbRn1EPHTqk1mhjLG3atFFrbH2mPrP13aysLGO+ZMkSv7fj9XrVtlatWhlz2/uhna9NmzZVa7R+Ecjnhe0cP3DggDEvKChQa2xjRLm5uX7ltrbi4mK1pqioSG3TaOMVgXwmaMdNRKSwsNCYn+h9Nt+0BQAAAAAAAAAXYdAWAAAAAAAAAFyEQVsAAAAAAAAAcBEGbQEAAAAAAADARRi0BQAAAAAAAAAXCa7tgraZ57QZ7nr27KnWtG3b1pjbZhaNi4sz5jNnzlRrtm7dasx79eql1mgzEK5evVqt0WYavPnmm9UabXZT26x8kZGRatvcuXON+bJly9Sa3/72t8a8U6dOas0rr7xizH/44Qe15sEHHzTmtpkTY2JijHnz5s3Vmo0bNxrz2NhYtQY/Ky0tVduCgoKMeUVFhd81tuuJNruiti4R+0zB2vqCg/XLn1YTyGyV2nULJy6QWeVt70dycrIxb9KkiVrzxRdfqG3+OtGZRY/18MMPq21av9VmRBYRGTFixAnv01G2/qexXWu09SUkJPi9nfqOfmFHvzj7+kV+fr7apt1X255HNOHh4Wqbdp9iO79t90oabX2264KtTbv3su13eXm539vRjvf27dvVmm7duhnzQO5lceK06+SqVavUGu390GZ6F9GfuW0Ced9t/U9rKykpUWtatGhhzPPy8tQarS01NVWt2bRpk9pWn+Xm5qpt2nXcdp9iux5pcnJyjLntnkcbJ9Oukza281i7Jts+y7R9s23H1je1ew5tOyIiUVFRxjwxMVGt0caIQkJC1BrteNvuuyIiIoy57Z5C2862bdvUmtrgm7YAAAAAAAAA4CIM2gIAAAAAAACAizBoCwAAAAAAAAAuwqAtAAAAAAAAALgIg7YAAAAAAAAA4CIM2gIAAAAAAACAiwTXdsHs7Gy1rbi42JivXbtWrfF6vcY8Li5OrZk9e7Yx/+KLL9Sarl27GvPFixerNZs3bzbmERERao32erZu3arWJCcnG/PIyEi/tyMikpiYaMw7deqk1mRlZRnzgwcPqjWNGzc25vn5+WrNTz/9ZMyDgoLUmpiYGGNeVVWl1hQUFBjzAwcOqDX4me3YBsLj8Rhzx3H8XleDBvrfmLTt2Nj2IZD9rqioMObh4eH+7dhxtoOfBXKcOnbsqLa1aNHCmOfl5ak12udCUVGRfzsWoGbNmqltvXr1UtvCwsKMeZ8+fU54n2rD9t4Fch3S1teyZUu/11Xf0S/oF8db35naL0JDQ4257bhq9xa281sTEhKitpWXl/u9Pm2/tdcpIlJZWWnMA7lPEhEJDjY/KmrbEdHPVdv7oG3H9hylHW/bvtneIxxfamqq2rZnzx5jrl1XRfTnNu18EKnb+21tXSL2a67teVyjff5p4wEiIrt27TLm2jP/2aqwsNCvPFDaOWa7rmjXo6ioKLVGO78CuX7Zxlq0zz9bvwh0Wxpt/Gj37t1qjfZ5ZrtuaMfO9lq19dlqtH5uez21wTdtAQAAAAAAAMBFGLQFAAAAAAAAABdh0BYAAAAAAAAAXIRBWwAAAAAAAABwEQZtAQAAAAAAAMBF9CnWjtG+fXu1bcyYMcbcNkuaNlPc/v371ZprrrnGmKelpak1q1evNuatWrVSa5o3b27M582bp9Z07drVmDdq1Eit0WbMtImPj1fb2rRpY8wPHjyo1nTq1MmY2/ZNW1+XLl3Ums6dOxtz2+y8kZGRxtw2K2zbtm2Nec+ePdUa/EybRTJQgcxirrHtWyCzIgcym7StRptJUptF/Uxmez/q8pwIZDsLFy6ss+27wSuvvKK2tWvXTm0bMmTIydidWrNdxwPpz9r6OnTo4Pe6Thb6xalDv7Cvz039oi5p57etf2mzQ2uzttvYZs7W9sE2Q73Gdi5obbYa2z5o51Agr9V2fkdHRxvzDRs2qDXae2d7PYH0I/ysZcuWapt23G0zuoeGhhrzsLAwtUY7j2zb0dieqwOZVd62D1u2bDHm2rOriEh2drYxj42NVWsaNmxozHNyctQa1E5xcbFfuc2hQ4dOdHdwFuGbtgAAAAAAAADgIgzaAgAAAAAAAICLMGgLAAAAAAAAAC7CoC0AAAAAAAAAuAiDtgAAAAAAAADgIgzaAgAAAAAAAICLBNd2wfz8fLVt3rx5xjwsLEyt6dSpkzEvLS1Va5YsWeJ3TUREhDGPjo5WayoqKox5enq6WpObm2vMIyMj1RpNQUGB2vbjjz+qbcHB5rezSZMmfu9DcnKy2paammrMg4KC1Jrt27cb84YNG6o12vu6detWtUZrW7dunVpT3zmOc0q2Y3t//WXbZ4/H4/f6Atm3QI5bgwb637kqKyuNeV0et/riVJ2TgWzHdn7Nnj3bmDdr1kytefzxx43522+/7d+OHcef/vQnY37ppZeqNc8//7zalpWVdcL75Cba5198fPwp3hMd/YJ+carVh35xKtj6hPa5vmvXLr+3Y7tH0PYhJCTE7/XZ7iuqqqrUNo2t/2v7rd3zHG99mtjYWGNue+7Rjo/tfQhk3/Az27mnHfeioiK1RntOt/WLsrIyY24797XzOCoqSq3RxgNE9GdU2+fismXLjHnfvn3Vmj179hhz7douol/fc3Jy1BoA7sY3bQEAAAAAAADARRi0BQAAAAAAAAAXYdAWAAAAAAAAAFyEQVsAAAAAAAAAcBEGbQEAAAAAAADARfSpB48RExOjtu3fv9+8csvMhgMHDjTmK1asUGuWLl1qzA8cOKDW9O7d25jn5eWpNdpMlrbZdmfOnGnM09PT1ZqWLVsac9vsl7t371bbtNekbUdEn/3SNmtnbm6uMbfNwLl+/XpjfvjwYbVGm/35s88+U2tCQ0ONeatWrdSa+k6bCTeQGcS1GVlF9H4RCNs5rs1Ma5vFNZCZj+uaNpOybaZdzana55MlIyNDbdPOMds1+dChQ8a8sLBQrdGubSUlJWqN1paWlqbW3Hvvvcbcdp3at2+fMR80aJBaM2nSJGP+5ZdfqjW/+93v1DY3C+T812attr3fpxr9gn5xIs7UfnEq2I6ddoy2b9/u93a0/iWiPyvl5+erNbb7Ho12L2K7T9KOga3Otj6v12vMw8LC1JrIyEhjvmvXLr/3zXaPaXs2xfElJCSobdozmHbui4h06tTJmNvOFe1zUdu+iN6XoqOj1Rrb+rRraOfOndWaWbNmGXPtudq2D7YxCc5x4MzDN20BAAAAAAAAwEUYtAUAAAAAAAAAF2HQFgAAAAAAAABchEFbAAAAAAAAAHARBm0BAAAAAAAAwEUYtAUAAAAAAAAAFwmu7YI//vij2hYREWHMKysr1Zr33nvPmAcFBak1HTt2NOZ79uxRa/bu3WvMV61apdYMHTrUmO/fv1+tSU5ONuZ5eXlqzerVq415Tk6OWhMSEqK2hYeHG/Ndu3apNdqx016PiP6+5ubmqjXNmzc35vv27VNr1q5da8ybNWum1rRq1cqYv/POO2oNaqdBA/PfeGz93OPx+LUuW5utpqqqyu99sHEcx+990NiuaWeq1NRUv9sSExPVmpiYGGNeXl6u1mjXUdu5smPHDmM+bdo0tUb7LBk4cKBa06tXL2PeuXNntebbb7815vfee69aU1ZWprZ5vV5jXlpaqta4WVFRkTGfN2/eKd4THf2CfnGq1Yd+UZe0z/tA7gNs9+8a7fyxtdn6a8OGDY257b6roqLCmAdyDGx1tvsh7dhFRkaqNU2bNjXmJSUlak1oaKgxDw7WH2+1GtROQkKC2qadEwcPHlRrYmNjjbntPdSeXW3v7aFDh4x5YWGhWhPIPb9NQUGBMdf2TUT/bLbtd5MmTYz5+vXrLXsHwM34pi0AAAAAAAAAuAiDtgAAAAAAAADgIgzaAgAAAAAAAICLMGgLAAAAAAAAAC7CoC0AAAAAAAAAuIg+NeMxsrKyAmqrj15//fXTvQtArTmOU2fr2r17t9rWrl07Y67NVCyiz3pqm6k8JCTE7xpbm3Z8bLMv22at9Xc7QUFBdbau+uLVV189Jdtp1KiR2ta8eXNjrs3GbauxzbqdkpJizHv16qXWREdHG/PZs2erNW+99ZYx37Fjh1pjU1paGlCdW2kzjN9zzz1qzV/+8peTtTtG9Av6xalWH/pFXdI+b8vKytQa7R4mkJnj//e//6ltMTExxnzfvn1qjXYvYrvv8nddIva+rLXZ7ru0/Tt8+LBas2zZMrXN3+3Yjk8g7yt+FhUVpbYVFRUZ8/j4eL+3ExYWprZp/dl2jicmJhrz/fv3qzWRkZF+ry8hIUGtSUtLM+a2vqSdr7Ya7bMUQP3FJxcAAAAAAAAAuAiDtgAAAAAAAADgIgzaAgAAAAAAAICLMGgLAAAAAAAAAC7CoC0AAAAAAAAAuAiDtgAAAAAAAADgIsGnewcAuEdcXJzaFhkZacyDg/XLSEJCgjFv0ED/e5HWFhISotYEorKyUm0LCgoy5jt27FBrIiIijHlaWpp/Oyb241NVVeX3+s5UBw8eDKgNZ56tW7ca83/+85+ndkdcgH6Bo862fhEeHm7MPR6PWqN93truhzSPP/643zUInOM4xtx2DxXI+4qftW3bVm3bsmWLMQ8LC/N7O7b3ULvfLikpUWsWLlxozK+55hq1xvZ889lnnxnzQJ5vbOdkYWGhMdeOtYjIggUL1DYA9RPftAUAAAAAAAAAF2HQFgAAAAAAAABchEFbAAAAAAAAAHARBm0BAAAAAAAAwEUYtAUAAAAAAAAAF/E42tSbxy5omXkVqM9q2QWM3NAvtH0I5HU99dRTapvX6zXmubm5ak1ISIjf+6DNrlpQUKDW2F6rdnwqKirUmqqqKmNeVlam1sTHxxvzpUuXqjUff/yx2na61fd+AZwM9AugpkD7xanqE88884zaps1EP2vWLLVG++wO5PWcyDXlbPfoo48a89atW6s1r7/+ujGfM2dOnezTUWfqZ0VwcLDapt1Xa/f1Ivr9dlpamlqzbds2Y968eXO1ZuvWrWobTp0ztV8AJ6I2/YJv2gIAAAAAAACAizBoCwAAAAAAAAAuwqAtAAAAAAAAALgIg7YAAAAAAAAA4CIM2gIAAAAAAACAizBoCwAAAAAAAAAu4nEcxzndOwEAAAAAAAAAOIJv2gIAAAAAAACAizBoCwAAAAAAAAAuwqAtAAAAAAAAALgIg7YAAAAAAAAA4CIM2gIAAAAAAACAizBoCwAAAAAAAAAuwqAtAAAAAAAAALgIg7YAAAAAAAAA4CIM2gIAAAAAAACAi/x/QtTz4O+CnPcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 7 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 7 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worthy-capybara-16</strong> at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1/runs/wied9zjf' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1/runs/wied9zjf</a><br> View project at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-1</a><br>Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250308_165034-wied9zjf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-2"
      ],
      "metadata": {
        "id": "EwxSLCyuiCpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Activation function (Sigmoid)\n",
        "def act(x):\n",
        "  return 1/(1+np.exp(-x))  # Converts input into a range between 0 and 1\n",
        "\n",
        "# Derivative of Sigmoid, used in backpropagation to compute gradients\n",
        "def act_der(x):\n",
        "  return act(x)*(1-act(x))\n",
        "\n",
        "# Softmax function, converts output layer activations into probabilities\n",
        "def soft_func(x):\n",
        "  x = x-np.max(x) # Normalize values to prevent overflow\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
        "\n",
        "# Initialize neural network weights and biases\n",
        "def init_net(n_layers, nodes, init_m, inp_dim, out_dim):\n",
        "  \"\"\"\n",
        "    Initializes the neural network weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - init_m: Weight initialization method ('rand' or 'xav')\n",
        "    - inp_dim: Input dimension\n",
        "    - out_dim: Output dimension\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary containing initialized weights and biases\n",
        "    \"\"\"\n",
        "  size = [inp_dim]+nodes+[out_dim]  # Define the structure of the network\n",
        "  wts = {}  # Dictionary to store weights and biases\n",
        "\n",
        "  if init_m == 'rand': # Random weight initialization\n",
        "    for i in range(1, n_layers+2):\n",
        "      wts['W'+str(i)] = np.random.randn(size[i],size[i-1])\n",
        "      wts['b'+str(i)] = np.random.randn(size[i],1)\n",
        "  elif init_m == 'xav': # Xavier initialization for better weight scaling\n",
        "    for i in range(1, n_layers+2):\n",
        "      wts['W'+str(i)] = np.random.randn(size[i],size[i-1])*(np.sqrt(2/size[i-1]))\n",
        "      wts['b'+str(i)] = np.random.randn(size[i],1)*(np.sqrt(2/size[i-1]))\n",
        "\n",
        "  return wts\n",
        "\n",
        "# Forward propagation - calculates activations layer by layer\n",
        "def forw_prop(inp,wts, n_layers, inp_dim):\n",
        "  \"\"\"\n",
        "    Performs forward propagation.\n",
        "\n",
        "    Parameters:\n",
        "    - inp: Input data\n",
        "    - wts: Weights dictionary\n",
        "    - n_layers: Number of hidden layers\n",
        "    - inp_dim: Input dimension\n",
        "\n",
        "    Returns:\n",
        "    - acts: Dictionary storing activations\n",
        "    - out: Dictionary storing outputs at each layer\n",
        "    - pred: Output probabilities from the softmax layer\n",
        "    \"\"\"\n",
        "  acts = {} # Store activation values\n",
        "  acts['a0'] = np.zeros((inp_dim,1)) # Initialize first activation\n",
        "  out = {'h0':inp} # Store input layer values\n",
        "\n",
        "  for i in range(1,n_layers+1): # Iterate through hidden layers\n",
        "    acts['a'+str(i)]= np.dot(wts['W'+str(i)], out['h'+str(i-1)]) + wts['b'+str(i)]\n",
        "    out['h'+str(i)]= act(acts['a'+str(i)]) # Apply activation function\n",
        "  # Output layer processing\n",
        "  acts['a'+str(n_layers+1)]= np.dot(wts['W'+str(n_layers+1)], out['h'+str(n_layers)]) + wts['b'+str(n_layers+1)]\n",
        "  pred = soft_func(acts['a'+str(n_layers+1)]) # Apply softmax for probabilities\n",
        "\n",
        "  return acts, out, pred\n",
        "\n",
        "# Backpropagation - calculates gradients for weight updates\n",
        "def back_prop(inp, true_lbl, n_layers, wts, inp_dim):\n",
        "  \"\"\"\n",
        "    Performs backpropagation to compute gradients.\n",
        "\n",
        "    Parameters:\n",
        "    - inp: Input data\n",
        "    - true_lbl: True labels\n",
        "    - n_layers: Number of hidden layers\n",
        "    - wts: Weights dictionary\n",
        "    - inp_dim: Input dimension\n",
        "\n",
        "    Returns:\n",
        "    - grad: Dictionary containing gradients for weight and bias updates\n",
        "    \"\"\"\n",
        "  m = inp.shape[0] # Number of examples\n",
        "  acts, out, pred = forw_prop(inp, wts, n_layers, inp_dim)\n",
        "  grad = {} # Store gradients\n",
        "  err = {} # Store error terms\n",
        "  err['a'+str(n_layers+1)] = -1*(true_lbl.T-pred) # Compute error for output layer\n",
        "\n",
        "  for i in range(n_layers+1,0,-1): # Iterate backwards through layers\n",
        "    grad['W'+str(i)]= (1/m)*np.dot(err['a'+str(i)], out['h'+str(i-1)].T) # Compute weight gradient\n",
        "    grad['b'+str(i)]= (1/m)*np.mean(err['a'+str(i)], axis=1, keepdims=True) # Compute bias gradient\n",
        "    if i>1:\n",
        "      err['h'+str(i-1)]= np.dot(wts['W'+str(i)].T, err['a'+str(i)]) # Propagate error backward\n",
        "      err['a'+str(i-1)]= err['h'+str(i-1)]*act_der(acts['a'+str(i-1)]) # Apply derivative of activation\n",
        "  return grad\n",
        "\n",
        "# Gradient Descent with loss tracking\n",
        "def upd_wts(lr, wts, inp, true_lbl, n_layers, inp_dim):\n",
        "  wandb.init(project='DA6401_Assignment-1', name='GradientDescent')\n",
        "  losses= []  # Store losses for each epoch\n",
        "  for e in range(100): # Train for 100 epochs\n",
        "    grad = back_prop(inp, true_lbl, n_layers, wts, inp_dim)\n",
        "    for i in range (1, n_layers+2):\n",
        "      wts['W'+str(i)] -= lr*grad['W'+str(i)] # Update weights\n",
        "      wts['b'+str(i)] -= lr*grad['b'+str(i)] # Update biases\n",
        "\n",
        "    # Compute loss for this epoch\n",
        "    acts, out, pred = forw_prop(inp, wts, n_layers, inp_dim)\n",
        "    loss = -np.mean(np.sum(true_lbl.T*np.log(pred),axis=0))\n",
        "    losses.append(loss)\n",
        "    wandb.log({'Epoch': e+1, 'Loss': loss})\n",
        "    print(f'Epoch {e+1}, Loss: {loss}')\n",
        "\n",
        "  # Plot the loss function over epochs\n",
        "  plt.plot(losses)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training Loss over Epochs')\n",
        "  plt.show()\n",
        "\n",
        "  wandb.finish()\n",
        "  return wts, losses\n",
        "\n",
        "# Compute accuracy of the model\n",
        "def calc_acc(test_inp, test_lbl, wts, n_layers, inp_dim):\n",
        "  acts, out, pred = forw_prop(test_inp, wts, n_layers, inp_dim)\n",
        "  assert test_lbl.shape == pred.shape # Ensure shapes match\n",
        "  test_lbl = np.argmax(test_lbl, axis=0) # Convert one-hot labels to index\n",
        "  pred = np.argmax(pred, axis=0) # Get predicted class\n",
        "  correct = np.sum(test_lbl==pred) # Count correct predictions\n",
        "  acc = correct/test_lbl.shape[0] # Compute accuracy\n",
        "\n",
        "  wandb.log({'Test Accuracy': acc})\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "tAUUYyAYtceR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-3"
      ],
      "metadata": {
        "id": "W9evn-A4qwj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(x,func):\n",
        "    \"\"\"\n",
        "    Applies the specified activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array\n",
        "    - func: Activation function ('sigmoid', 'relu', 'tanh')\n",
        "\n",
        "    Returns:\n",
        "    - Transformed input based on the activation function\n",
        "    \"\"\"\n",
        "    if func =='sigmoid':\n",
        "        return 1/(1+np.exp(-x))\n",
        "    elif func =='relu':\n",
        "        return np.maximum(0,x)\n",
        "    elif func =='tanh':\n",
        "        return np.tanh(x)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "def activation_derivative(x,func):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array\n",
        "    - func: Activation function ('sigmoid', 'relu', 'tanh')\n",
        "\n",
        "    Returns:\n",
        "    - Derivative of the activation function\n",
        "    \"\"\"\n",
        "    if func =='sigmoid':\n",
        "        sig=activation(x,'sigmoid')\n",
        "        return sig*(1-sig)\n",
        "    elif func =='relu':\n",
        "        return np.where(x>0,1,0)\n",
        "    elif func =='tanh':\n",
        "        return 1-np.tanh(x)**2\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "def soft_func(x):\n",
        "    \"\"\"\n",
        "    Applies the softmax function for output layer activation.\n",
        "    Parameters:\n",
        "    - x: Input array\n",
        "\n",
        "    Returns:\n",
        "    - Softmax probabilities\n",
        "    \"\"\"\n",
        "    x = x-np.max(x)  # Prevent numerical instability\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "\n",
        "def init_net(n_layers,nodes,init_m,inp_dim,out_dim):\n",
        "    \"\"\"\n",
        "    Initializes the neural network weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - init_m: Weight initialization method ('rand' or 'xav')\n",
        "    - inp_dim: Input dimension\n",
        "    - out_dim: Output dimension\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary containing initialized weights and biases\n",
        "    \"\"\"\n",
        "    size = [inp_dim]+nodes+[out_dim]  # Define network structure\n",
        "    wts = {}\n",
        "    if init_m =='rand':  # Random initialization\n",
        "        for i in range(1,n_layers+2):\n",
        "            wts['W'+str(i)] = np.random.randn(size[i],size[i-1])\n",
        "            wts['b'+str(i)] = np.random.randn(size[i],1)\n",
        "    elif init_m =='xav':  # Xavier initialization\n",
        "        for i in range(1,n_layers+2):\n",
        "            wts['W'+str(i)] = np.random.randn(size[i],size[i-1])*(np.sqrt(2/size[i-1]))\n",
        "            wts['b'+str(i)] = np.zeros((size[i],1))  # Initialize biases to zero\n",
        "    return wts\n",
        "\n",
        "def forw_prop(inp, wts,n_layers, activation_func, inp_dim):\n",
        "    \"\"\"\n",
        "    Performs forward propagation.\n",
        "\n",
        "    Parameters:\n",
        "    - inp: Input data\n",
        "    - wts: Weights dictionary\n",
        "    - n_layers: Number of hidden layers\n",
        "    - activation_func: Activation function for hidden layers\n",
        "    - inp_dim: Input dimension\n",
        "\n",
        "    Returns:\n",
        "    - acts: Dictionary storing activations\n",
        "    - out: Dictionary storing outputs at each layer\n",
        "    - pred: Output probabilities from the softmax layer\n",
        "    \"\"\"\n",
        "    acts,out = {},{}\n",
        "    acts['a0'] = np.zeros((inp_dim,1))  # Initialize first activation\n",
        "    out['h0'] = inp\n",
        "\n",
        "    for i in range(1, n_layers+1):  # Process hidden layers\n",
        "        acts['a'+str(i)] = np.dot(wts['W'+str(i)],out['h'+str(i-1)])+wts['b'+str(i)]\n",
        "        out['h'+str(i)] = activation(acts['a'+str(i)],activation_func)\n",
        "\n",
        "    # Compute output layer activations\n",
        "    acts['a'+str(n_layers+1)] = np.dot(wts['W'+str(n_layers+1)],out['h'+str(n_layers)])+wts['b'+str(n_layers+1)]\n",
        "    pred = soft_func(acts['a'+str(n_layers+1)])  # Apply softmax\n",
        "\n",
        "    return acts,out,pred\n",
        "\n",
        "def back_prop(inp,true_lbl,n_layers,wts,activation_func,inp_dim):\n",
        "    \"\"\"\n",
        "    Performs backpropagation to compute gradients.\n",
        "\n",
        "    Parameters:\n",
        "    - inp: Input data\n",
        "    - true_lbl: True labels\n",
        "    - n_layers: Number of hidden layers\n",
        "    - wts: Weights dictionary\n",
        "    - activation_func: Activation function used\n",
        "    - inp_dim: Input dimension\n",
        "\n",
        "    Returns:\n",
        "    - grad: Dictionary containing gradients for weight and bias updates\n",
        "    \"\"\"\n",
        "    m = inp.shape[1]  # Number of training samples\n",
        "    acts,out,pred = forw_prop(inp,wts,n_layers,activation_func,inp_dim)\n",
        "    grad,err = {},{}\n",
        "\n",
        "    # Compute error for output layer\n",
        "    err['a'+str(n_layers+1)]= pred-true_lbl.T\n",
        "\n",
        "    for i in range(n_layers+1, 0,-1):  # Iterate backward through layers\n",
        "        grad['W'+str(i)] = (1/m)*np.dot(err['a'+str(i)],out['h'+str(i-1)].T)  # Compute weight gradient\n",
        "        grad['b'+str(i)] = (1/m)*np.mean(err['a'+str(i)],axis=1,keepdims=True)  # Compute bias gradient\n",
        "        if i>1:\n",
        "            err['h'+str(i-1)] = np.dot(wts['W'+str(i)].T,err['a'+str(i)])  # Compute hidden layer error\n",
        "            err['a'+str(i-1)] = err['h'+str(i-1)]*activation_derivative(acts['a'+str(i-1)],activation_func)  # Apply activation derivative\n",
        "    return grad\n",
        "\n",
        "def calc_acc(test_inp,test_lbl, wts, n_layers, activation_func,inp_dim):\n",
        "    \"\"\"\n",
        "    Computes accuracy of the model on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - test_inp: Test input data\n",
        "    - test_lbl: Test labels (one-hot encoded)\n",
        "    - wts: Weights dictionary\n",
        "    - n_layers: Number of hidden layers\n",
        "    - activation_func: Activation function used\n",
        "    - inp_dim: Input dimension\n",
        "\n",
        "    Returns:\n",
        "    - acc: Accuracy of the model\n",
        "    \"\"\"\n",
        "    acts,out,pred = forw_prop(test_inp,wts,n_layers,activation_func,inp_dim)  # Get model predictions\n",
        "    test_lbl = np.argmax(test_lbl,axis=0)  # Convert one-hot labels to class indices\n",
        "    pred = np.argmax(pred,axis=0)  # Get predicted class indices\n",
        "    acc = np.mean(test_lbl==pred)  # Compute accuracy\n",
        "    wandb.log({'Test Accuracy':acc})  # Log accuracy in WandB\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "2jV6-Axej9I9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "WTWN9ldu3Kca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gd(lr, x_train,y_train, x_valid,y_valid, epochs, activation_func, n_layers,nodes, weight_init,batch_size, inp_dim,out_dim):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent (SGD) optimizer.\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate\n",
        "    - x_train, y_train: Training data and labels\n",
        "    - X_valid, Y_valid: Validation data and labels\n",
        "    - epochs: Number of training epochs\n",
        "    - activation_func: Activation function ('relu', 'tanh', etc.)\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes per hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Mini-batch size\n",
        "    - inp_dim, out_dim: Input and output dimensions\n",
        "\n",
        "    Returns:\n",
        "    - Optimized weights dictionary\n",
        "    \"\"\"\n",
        "    wts = init_net(n_layers,nodes,weight_init,inp_dim,out_dim) # Initialize weights\n",
        "    losses = [] # Track loss per epoch\n",
        "    n = x_train.shape[0] # Training set size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.arange(n)\n",
        "        np.random.shuffle(idx) #Shuffle data\n",
        "        cnt = 0\n",
        "        grad_accu = {k:np.zeros_like(v) for k,v in wts.items()} # Initialize accumulated gradients\n",
        "\n",
        "        for j in range(n):\n",
        "            cnt+=1\n",
        "            x = x_train[idx[j],:].reshape(-1,1) # Get input sample\n",
        "            y_true = y_train[idx[j],:].reshape(-1,1) # Get corresponding label\n",
        "            grad = back_prop(x,y_true,n_layers,wts,activation_func,inp_dim) # Compute gradients\n",
        "\n",
        "            for k in range(1,n_layers+2): # Accumulate gradients\n",
        "                grad_accu['W'+str(k)] += grad['W'+str(k)]\n",
        "                grad_accu['b'+str(k)] += grad['b'+str(k)]\n",
        "\n",
        "            if cnt%batch_size == 0: # Update weights when batch is processed\n",
        "                for i in range(1,n_layers+2):\n",
        "                    wts['W'+str(i)] -= lr*grad_accu['W'+str(i)] # Update weight W\n",
        "                    wts['b'+str(i)] -= lr*grad_accu['b'+str(i)] # Update bias b\n",
        "                grad_accu = {k:np.zeros_like(v) for k,v in wts.items()} # Reset accumulated gradients\n",
        "\n",
        "        train_acc = calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim) # Compute training accuracy\n",
        "        acts, out, y_pred = forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim) # Forward pass\n",
        "        loss = -np.mean(np.sum(y_train.T*np.log(y_pred+1e-9),axis=0)) # Compute loss\n",
        "        losses.append(loss)\n",
        "        val_acc = calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim) # Compute validation accuracy\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Train Acc: {train_acc*100:.2f}%, Val Acc: {val_acc*100:.2f}%\")\n",
        "        wandb.log({'train_loss':loss,'train_acc':train_acc*100,'val_acc':val_acc*100,'epoch':epoch}) # Log metrics\n",
        "\n",
        "    plt.plot(losses) # Plot loss curve\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.show()\n",
        "\n",
        "    return wts # Return updated weights\n"
      ],
      "metadata": {
        "id": "jj7XzDQN15oL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Based Gradient Descent"
      ],
      "metadata": {
        "id": "wB1AiDSV6IPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(lr,beta,x_train,y_train,x_valid,y_valid,epochs,activation_func,n_layers,nodes,weight_init,batch_size,inp_dim,out_dim):\n",
        "    \"\"\"\n",
        "    Implements Momentum-based Gradient Descent following the update rule:\n",
        "        u_t = β * u_(t-1) + ∇w_t\n",
        "        w_(t+1) = w_t - η * u_t\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate (η)\n",
        "    - beta: Momentum coefficient (β) in the range [0,1)\n",
        "    - x_train, y_train: Training dataset\n",
        "    - X_valid, Y_valid: Validation dataset\n",
        "    - epochs: Number of training iterations\n",
        "    - activation_func: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Size of each mini-batch\n",
        "    - inp_dim: Number of input features\n",
        "    - out_dim: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Updated weights after training\n",
        "    \"\"\"\n",
        "    wts = init_net(n_layers,nodes,weight_init,inp_dim,out_dim)  # Initialize weights\n",
        "    losses = []  # Track loss per epoch\n",
        "    num_samples = x_train.shape[0]\n",
        "    prev_u = {key:np.zeros_like(value) for key,value in wts.items()}  # Initialize velocity\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.arange(num_samples)\n",
        "        np.random.shuffle(indices)  # Shuffle data\n",
        "        grad_accu = {key:np.zeros_like(value) for key,value in wts.items()}  # Gradient accumulation\n",
        "        num_processed = 0\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            num_processed+=1\n",
        "            x = x_train[indices[i],:].reshape(-1,1)  # Get input sample\n",
        "            y_actual = y_train[indices[i],:].reshape(-1,1)  # Get label\n",
        "            grad = back_prop(x,y_actual,n_layers,wts,activation_func,inp_dim)  # Compute gradients\n",
        "\n",
        "            for j in range(1,n_layers+2):  # Accumulate gradients\n",
        "                grad_accu['W'+str(j)] += grad['W'+str(j)]\n",
        "                grad_accu['b'+str(j)] += grad['b'+str(j)]\n",
        "\n",
        "            if num_processed%batch_size==0:  # Update weights after processing a batch\n",
        "                for k in range(1,n_layers+2):\n",
        "                    u_w = beta*prev_u['W'+str(k)]+grad_accu['W'+str(k)]\n",
        "                    u_b = beta*prev_u['b'+str(k)]+grad_accu['b'+str(k)]\n",
        "                    wts['W'+str(k)] -= lr*u_w\n",
        "                    wts['b'+str(k)] -= lr*u_b\n",
        "                    prev_u['W'+str(k)] = u_w\n",
        "                    prev_u['b'+str(k)] = u_b\n",
        "                grad_accu={key:np.zeros_like(value) for key,value in wts.items()}  # Reset accumulated gradients\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        acc = calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim)\n",
        "        acts,out,y_pred = forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim)\n",
        "        loss = -np.mean(np.sum(y_train.T*np.log(y_pred),axis=0))  # Cross-entropy loss\n",
        "        losses.append(loss)\n",
        "        val_acc = calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Training Accuracy: {acc*100:.2f}%, Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Log metrics in WandB\n",
        "        wandb.log({'train_loss':loss,'train_accuracy':acc*100,'Val_accuracy':val_acc*100,'epoch':epoch})\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return wts\n"
      ],
      "metadata": {
        "id": "KTvs_mTJ2DT4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesterov Accelerated Gradient Descent"
      ],
      "metadata": {
        "id": "6cCuUWpN5xB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(lr,x_train,y_train,x_valid,y_valid,epochs,activation_func,n_layers,nodes,weight_init,batch_size,inp_dim,out_dim,beta):\n",
        "    \"\"\"\n",
        "    Implements Nesterov Accelerated Gradient Descent (NAG):\n",
        "        u_t = β * u_(t-1) + ∇(w - η*β * u_(t-1))\n",
        "        w_(t+1) = w_t - η * u_t\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate (η)\n",
        "    - beta: Momentum coefficient (β) in the range [0,1)\n",
        "    - x_train, y_train: Training dataset\n",
        "    - X_valid, Y_valid: Validation dataset\n",
        "    - epochs: Number of training iterations\n",
        "    - activation_func: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Size of each mini-batch\n",
        "    - inp_dim: Number of input features\n",
        "    - out_dim: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Updated weights after training\n",
        "    \"\"\"\n",
        "    wts = init_net(n_layers,nodes,weight_init,inp_dim,out_dim)  # Initialize weights\n",
        "    prev_u = {key:np.zeros_like(value) for key,value in wts.items()}  # Initialize velocity\n",
        "    losses = []  # Track loss per epoch\n",
        "    num_samples = x_train.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.arange(num_samples)\n",
        "        np.random.shuffle(indices)  # Shuffle dataset\n",
        "        grad_accu = {key:np.zeros_like(value) for key,value in wts.items()}  # Gradient accumulation\n",
        "        num_processed=0\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            num_processed+=1\n",
        "            x = x_train[indices[i],:].reshape(-1,1)  # Get training example\n",
        "            y_actual = y_train[indices[i],:].reshape(-1,1)  # Get label\n",
        "\n",
        "            # Lookahead step: Compute gradients at future position\n",
        "            wts_lookahead = {key:wts[key]-lr*beta*prev_u[key] for key in wts}\n",
        "            grad = back_prop(x,y_actual,n_layers,wts_lookahead,activation_func,inp_dim)\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for j in range(1,n_layers+2):\n",
        "                grad_accu['W'+str(j)] += grad['W'+str(j)]\n",
        "                grad_accu['b'+str(j)] += grad['b'+str(j)]\n",
        "\n",
        "            # Perform update after batch\n",
        "            if num_processed%batch_size == 0:\n",
        "                for k in range(1, n_layers+2):\n",
        "                    u_w = beta*prev_u['W'+str(k)]+grad_accu['W'+str(k)]\n",
        "                    u_b = beta*prev_u['b'+str(k)]+grad_accu['b'+str(k)]\n",
        "                    wts['W'+str(k)] -= lr*u_w\n",
        "                    wts['b'+str(k)] -= lr*u_b\n",
        "                    prev_u['W'+str(k)] = u_w\n",
        "                    prev_u['b'+str(k)] = u_b\n",
        "                grad_accu = {key:np.zeros_like(value) for key,value in wts.items()}  # Reset gradients\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        acc = calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim)\n",
        "        acts,out,y_pred = forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim)\n",
        "        loss = -np.mean(np.sum(y_train.T*np.log(y_pred),axis=0))\n",
        "        losses.append(loss)\n",
        "        val_acc = calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Training Accuracy: {acc*100:.2f}%, Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Log metrics in WandB\n",
        "        wandb.log({'train_loss':loss,'train_accuracy':acc*100,'Val_accuracy':val_acc*100,'epoch':epoch})\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return wts\n"
      ],
      "metadata": {
        "id": "o_4dHdwz5vbJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSPROP"
      ],
      "metadata": {
        "id": "tYX12sWsquPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(lr,x_train,y_train,x_valid,y_valid,epochs,activation_func,n_layers,nodes,weight_init,batch_size,inp_dim,out_dim,beta,epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Implements RMSprop optimization:\n",
        "        v_t = β * v_(t-1) + (1-β) * (∇w_t)^2\n",
        "        w_(t+1) = w_t - η*(∇w_t / (√v_t + ε))\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate (η)\n",
        "    - beta: Decay rate (β) in the range [0,1)\n",
        "    - epsilon: Small constant to prevent division by zero\n",
        "    - x_train, y_train: Training dataset\n",
        "    - X_valid, Y_valid: Validation dataset\n",
        "    - epochs: Number of training iterations\n",
        "    - activation_func: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Size of each mini-batch\n",
        "    - inp_dim: Number of input features\n",
        "    - out_dim: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Updated weights after training\n",
        "    \"\"\"\n",
        "    wts=init_net(n_layers,nodes,weight_init,inp_dim,out_dim)  # Initialize weights\n",
        "    v={key:np.zeros_like(value) for key,value in wts.items()}  # Init squared gradient accumulator\n",
        "    losses=[]  # Track loss per epoch\n",
        "    num_samples=x_train.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices=np.arange(num_samples)\n",
        "        np.random.shuffle(indices)  # Shuffle dataset\n",
        "        grad_accu={key:np.zeros_like(value) for key,value in wts.items()}  # Gradient accumulation\n",
        "        num_processed=0\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            num_processed+=1\n",
        "            x=x_train[indices[i],:].reshape(-1,1)  # Get training example\n",
        "            y_actual=y_train[indices[i],:].reshape(-1,1)  # Get label\n",
        "            grad=back_prop(x,y_actual,n_layers,wts,activation_func,inp_dim)\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for j in range(1,n_layers+2):\n",
        "                grad_accu['W'+str(j)]+=grad['W'+str(j)]\n",
        "                grad_accu['b'+str(j)]+=grad['b'+str(j)]\n",
        "\n",
        "            # Perform update after batch\n",
        "            if num_processed%batch_size==0:\n",
        "                for k in range(1,n_layers+2):\n",
        "                    v['W'+str(k)]=beta*v['W'+str(k)]+(1-beta)*(grad_accu['W'+str(k)]**2)\n",
        "                    v['b'+str(k)]=beta*v['b'+str(k)]+(1-beta)*(grad_accu['b'+str(k)]**2)\n",
        "                    wts['W'+str(k)]-=lr*(grad_accu['W'+str(k)]/(np.sqrt(v['W'+str(k)])+epsilon))\n",
        "                    wts['b'+str(k)]-=lr*(grad_accu['b'+str(k)]/(np.sqrt(v['b'+str(k)])+epsilon))\n",
        "                grad_accu={key:np.zeros_like(value) for key,value in wts.items()}  # Reset gradients\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        acc=calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim)\n",
        "        acts,out,y_pred=forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim)\n",
        "        loss=-np.mean(np.sum(y_train.T*np.log(y_pred),axis=0))\n",
        "        losses.append(loss)\n",
        "        val_acc=calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Training Accuracy: {acc*100:.2f}%, Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Log metrics in WandB\n",
        "        wandb.log({'train_loss':loss,'train_accuracy':acc*100,'Val_accuracy':val_acc*100,'epoch':epoch})\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return wts\n"
      ],
      "metadata": {
        "id": "SuTOLZC7_AMy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADAM"
      ],
      "metadata": {
        "id": "ohqRIfnSi88z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(lr,x_train,y_train,x_valid,y_valid,epochs,activation_func,n_layers,nodes,weight_init,batch_size,inp_dim,out_dim,beta1,beta2,epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Implements Adam optimization:\n",
        "        m_t = β1*m_(t-1) + (1-β1)*∇w_t\n",
        "        hat{m_t} = m_t / (1-β1^t)\n",
        "        v_t = β2*v_(t-1) + (1-β2)*(∇w_t)^2\n",
        "        hat{v_t} = v_t / (1-β2^t)\n",
        "        w_(t+1) = w_t - η*(hat{m_t} / (√(hat{v_t}) +ε) )\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate (η)\n",
        "    - beta1: Decay rate for first moment estimate (β1)\n",
        "    - beta2: Decay rate for second moment estimate (β2)\n",
        "    - epsilon: Small constant to prevent division by zero\n",
        "    - x_train, y_train: Training dataset\n",
        "    - X_valid, Y_valid: Validation dataset\n",
        "    - epochs: Number of training iterations\n",
        "    - activation_func: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Size of each mini-batch\n",
        "    - inp_dim: Number of input features\n",
        "    - out_dim: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Updated weights after training\n",
        "    \"\"\"\n",
        "    wts=init_net(n_layers,nodes,weight_init,inp_dim,out_dim)  # Initialize weights\n",
        "    m={key:np.zeros_like(value) for key,value in wts.items()}  # First moment estimate\n",
        "    v={key:np.zeros_like(value) for key,value in wts.items()}  # Second moment estimate\n",
        "    losses=[]  # Track loss per epoch\n",
        "    num_samples=x_train.shape[0]\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        indices=np.arange(num_samples)\n",
        "        np.random.shuffle(indices)  # Shuffle dataset\n",
        "        grad_accu={key:np.zeros_like(value) for key,value in wts.items()}  # Gradient accumulation\n",
        "        num_processed=0\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            num_processed+=1\n",
        "            x=x_train[indices[i],:].reshape(-1,1)  # Get training example\n",
        "            y_actual=y_train[indices[i],:].reshape(-1,1)  # Get label\n",
        "            grad=back_prop(x,y_actual,n_layers,wts,activation_func,inp_dim)\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for j in range(1,n_layers+2):\n",
        "                grad_accu['W'+str(j)]+=grad['W'+str(j)]\n",
        "                grad_accu['b'+str(j)]+=grad['b'+str(j)]\n",
        "\n",
        "            # Perform update after batch\n",
        "            if num_processed%batch_size==0:\n",
        "                for k in range(1,n_layers+2):\n",
        "                    m['W'+str(k)] = beta1*m['W'+str(k)]+(1-beta1)*grad_accu['W'+str(k)]\n",
        "                    m['b'+str(k)] = beta1*m['b'+str(k)]+(1-beta1)*grad_accu['b'+str(k)]\n",
        "\n",
        "                    v['W'+str(k)] = beta2*v['W'+str(k)]+(1-beta2)*(grad_accu['W'+str(k)]**2)\n",
        "                    v['b'+str(k)] = beta2*v['b'+str(k)]+(1-beta2)*(grad_accu['b'+str(k)]**2)\n",
        "\n",
        "                    m_hat_w = m['W'+str(k)]/(1-beta1**epoch)\n",
        "                    m_hat_b = m['b'+str(k)]/(1-beta1**epoch)\n",
        "                    v_hat_w = v['W'+str(k)]/(1-beta2**epoch)\n",
        "                    v_hat_b = v['b'+str(k)]/(1-beta2**epoch)\n",
        "\n",
        "                    wts['W'+str(k)] -= lr*(m_hat_w/(np.sqrt(v_hat_w)+epsilon))\n",
        "                    wts['b'+str(k)] -= lr*(m_hat_b/(np.sqrt(v_hat_b)+epsilon))\n",
        "                grad_accu={key:np.zeros_like(value) for key,value in wts.items()}  # Reset gradients\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        acc=calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        # Forward propagation for loss calculation\n",
        "        acts,out,y_pred=forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim)\n",
        "        loss=-np.mean(np.sum(y_train.T*np.log(y_pred),axis=0))\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        val_acc=calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        # Print epoch details\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Training Accuracy: {acc*100:.2f}%, Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Log metrics in WandB\n",
        "        wandb.log({'train_loss':loss,'train_accuracy':acc*100,'Val_accuracy':val_acc*100,'epoch':epoch})\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return wts\n"
      ],
      "metadata": {
        "id": "ydQ8T_P-i5R2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NADAM"
      ],
      "metadata": {
        "id": "u3v8Sd5orhQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(lr,x_train,y_train,x_valid,y_valid,epochs,activation_func,n_layers,nodes,weight_init,batch_size,inp_dim,out_dim,beta1,beta2,eps=1e-8):\n",
        "    \"\"\"\n",
        "    Implements NAdam optimization:\n",
        "\n",
        "    Parameters:\n",
        "    - lr: Learning rate (η)\n",
        "    - beta1: Decay rate for first moment estimate (β1)\n",
        "    - beta2: Decay rate for second moment estimate (β2)\n",
        "    - eps: Small constant to prevent division by zero\n",
        "    - x_train, y_train: Training dataset\n",
        "    - x_valid, y_valid: Validation dataset\n",
        "    - epochs: Number of training iterations\n",
        "    - activation_func: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    - n_layers: Number of hidden layers\n",
        "    - nodes: List of nodes in each hidden layer\n",
        "    - weight_init: Weight initialization method ('rand' or 'xav')\n",
        "    - batch_size: Size of each mini-batch\n",
        "    - inp_dim: Number of input features\n",
        "    - out_dim: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Updated weights after training\n",
        "    \"\"\"\n",
        "    # Initialize network weights\n",
        "    wts=init_net(n_layers,nodes,weight_init,inp_dim,out_dim)\n",
        "    losses=[]\n",
        "    num_samples=x_train.shape[0]\n",
        "\n",
        "    # Initialize moment estimates\n",
        "    m={key:np.zeros_like(value)for key,value in wts.items()}\n",
        "    v={key:np.zeros_like(value)for key,value in wts.items()}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices=np.arange(num_samples)\n",
        "        np.random.shuffle(indices)  # Shuffle data for stochastic updates\n",
        "        grad_accu={key:np.zeros_like(value)for key,value in wts.items()}\n",
        "        num_processed=0\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            num_processed+=1\n",
        "            x=x_train[indices[i],:].reshape(-1,1)\n",
        "            y_actual=y_train[indices[i],:].reshape(-1,1)\n",
        "\n",
        "            # Compute gradients via backpropagation\n",
        "            grad=back_prop(x,y_actual,n_layers,wts,activation_func,inp_dim)\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for j in range(1,n_layers+2):\n",
        "                grad_accu['W'+str(j)]+=grad['W'+str(j)]\n",
        "                grad_accu['b'+str(j)]+=grad['b'+str(j)]\n",
        "\n",
        "            # Update weights after processing a batch\n",
        "            if num_processed%batch_size==0:\n",
        "                for k in range(1,n_layers+2):\n",
        "                    # Compute first moment estimate (momentum)\n",
        "                    m['W'+str(k)]=beta1*m['W'+str(k)]+(1-beta1)*grad_accu['W'+str(k)]\n",
        "                    m['b'+str(k)]=beta1*m['b'+str(k)]+(1-beta1)*grad_accu['b'+str(k)]\n",
        "\n",
        "                    # Compute second moment estimate (RMSprop)\n",
        "                    v['W'+str(k)]=beta2*v['W'+str(k)]+(1-beta2)*(grad_accu['W'+str(k)]**2)\n",
        "                    v['b'+str(k)]=beta2*v['b'+str(k)]+(1-beta2)*(grad_accu['b'+str(k)]**2)\n",
        "\n",
        "                    # Bias correction for moments\n",
        "                    m_w_hat=m['W'+str(k)]/(1-beta1**(epoch+1))\n",
        "                    m_b_hat=m['b'+str(k)]/(1-beta1**(epoch+1))\n",
        "                    v_w_hat=v['W'+str(k)]/(1-beta2**(epoch+1))\n",
        "                    v_b_hat=v['b'+str(k)]/(1-beta2**(epoch+1))\n",
        "\n",
        "                    # Update weights using NAdam optimizer formula\n",
        "                    wts['W'+str(k)]-=lr/np.sqrt(v_w_hat+eps)*(beta1*m_w_hat+(1-beta1)*grad_accu['W'+str(k)]/(1-beta1**(epoch+1)))\n",
        "                    wts['b'+str(k)]-=lr/np.sqrt(v_b_hat+eps)*(beta1*m_b_hat+(1-beta1)*grad_accu['b'+str(k)]/(1-beta1**(epoch+1)))\n",
        "\n",
        "                # Reset accumulated gradients after batch update\n",
        "                grad_accu={key:np.zeros_like(value)for key,value in wts.items()}\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        acc=calc_acc(x_train,y_train,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        # Forward propagation for loss calculation\n",
        "        acts,out,y_pred=forw_prop(x_train.T,wts,n_layers,activation_func,inp_dim)\n",
        "        loss=-np.mean(np.sum(y_train.T*np.log(y_pred),axis=0))\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        val_acc=calc_acc(x_valid,y_valid,wts,n_layers,activation_func,inp_dim)\n",
        "\n",
        "        # Print epoch details\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Training Accuracy: {acc*100:.2f}%, Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Log metrics for visualization\n",
        "        wandb.log({'train_loss':loss,'train_accuracy':acc*100,'Val_accuracy':val_acc*100,'epoch':epoch})\n",
        "\n",
        "    # Plot loss curve over epochs\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return wts\n"
      ],
      "metadata": {
        "id": "jFMfb6hyrmJg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jLgOByvFnYn_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HvXMdnhvOgl"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}